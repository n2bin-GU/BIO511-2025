---
title: "Practical 3 — Workspace & Data Management"
author: "Stuti Jain, Tor Kling"
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: false
    theme: cosmo
execute:
  echo: true
  warning: false
  message: false
---

# Introduction

The goal of this practical is to build **good workspace habits** on the command line so your projects are **organized, reproducible, and HPC-friendly**. You’ll practice:

- Project layouts that keep data, scripts, and results separate  
- Using **symlinks** to reference large/shared data (don’t duplicate!)  
- Finding/searching files efficiently  
- Combining commands with **pipes** and **redirection**  
- Managing basic **file permissions**  
- Understanding how these habits translate to the **HPC filesystem**

> **Important**  
> We **reuse the dataset you created in Practical 1** — the FASTA, gene annotations, and expression data files. Here, we will reorganize them into a structured project layout. If some of them still have different names (such as an E_coli prefix), please rename them back to the original filenames for consistency with this practical.

::: {.callout-note title="Did You Know?"}
Most bioinformatics tools are tiny **LEGO bricks** on the CLI. You connect bricks with **pipes** `|` to build powerful, reproducible analyses.
:::

---

# Workspace Setup

In Practical 1 you created a folder called **CLI_intro/** and placed some mock files there (FASTA sequences, annotations, expression data).  
Now we will **expand this folder into a proper project structure** so that it’s easier to keep track of data, scripts, and results.

```bash
cd ~/CLI_intro

# Make a reproducible project layout
mkdir -p data refs results scripts logs

# Move the files you created in Practical 1 into the right places
mv sequences.fasta refs/
mv gene_annotations.csv data/
mv expression_data.csv data/
```

Your folder tree should now look like:

```
CLI_intro/
├── data/         # CSV annotation + expression data
├── refs/         # FASTA sequences
├── results/      # outputs you generate
├── scripts/      # scripts (used later)
└── logs/         # logs from runs
```

::: {.callout-tip title="Best Practice"}
Keep **raw data** read-only and **never** write outputs into `data/`. Use `results/` for generated files and `logs/` for run logs.
:::

---

# Symlinks: linking shared or large data

A **symbolic link (symlink)** points to a file by **path** (like a shortcut). This avoids unnecessary copies and keeps one **source of truth**.

```bash
# Example: link a shared reference into your existing refs/ folder
# (If your course provides a shared path on HPC, you'll link that path here.)
cd ~/CLI_intro

# Link target --> link name (re-using existing refs/ made above)
ln -s /proj/course/shared_refs/genome.fa refs/genome.fa   # example path
ls -l refs            # look for the arrow "genome.fa -> /proj/..."
```

::: {.callout-note title="When to use symlinks"}
- Large or shared files (reference genomes, annotation bundles)  
- Data living in shared HPC locations (`/proj/...`)  
- Multiple projects needing the **same** resource
:::

::: {.callout-warning title="Common Mistake"}
Don’t create **another** `refs/` folder somewhere else. Reuse the **existing** `CLI_intro/refs/` to keep your project tidy.
:::

---

# Finding & Searching Files

## Locate by name/type/size with `find`

```bash
cd ~/CLI_intro
find . -name "*.tsv"           # all TSV files
find data -type f -size +1M    # files > 1 MB in data/
find . -maxdepth 2 -type d     # directories up to depth 2
```

## Peek safely with `head`, `tail`, `less`

We assume **`data/metadata.tsv`** exists (or you can rename `expression_data.csv` to `metadata.tsv` for practice).

```bash
# Show a small slice so it's visibly different from the full file:
head -n 2 data/metadata.tsv
tail -n 2 data/metadata.tsv
less data/metadata.tsv   # press 'q' to quit
```

::: {.callout-note title="Defaults"}
By default, `head` and `tail` each print **10 lines** if you don’t specify `-n`.
:::

## Search inside files with `grep`

```bash
# Case-insensitive search for the word "case" in metadata
grep -i "case" data/metadata.tsv

# Count the number of FASTA headers (if you have a FASTA in refs/)
grep -c "^>" refs/sequences.fasta
```

::: {.callout-warning title="Common Mistake"}
If the command says “No such file or directory”, ensure you’re in `~/CLI_intro` and the file really exists **there**. Use `pwd` and `ls` to verify.
:::

## Counts and quick summaries with `wc`, `sort`, `uniq`

> Correct mental model:  
> `sort` **orders** lines (alphabetically/numerically). `uniq` only removes **adjacent** duplicates — so you almost always **sort first** before `uniq`.

```bash
# How many rows (lines) are in the metadata?
wc -l data/metadata.tsv

# STEP-BY-STEP (encouraged so you see each stage):
cut -f2 data/metadata.tsv              # column 2 (e.g., country) from a TAB-separated file
cut -f2 data/metadata.tsv | sort       # order values to cluster identical ones
cut -f2 data/metadata.tsv | sort | uniq -c   # collapse adjacent duplicates and count
```

Now that you’ve seen each stage, try the **pipeline** form that saves the result:

```bash
cut -f2 data/metadata.tsv | sort | uniq -c | sort -nr > results/country_counts.txt
head results/country_counts.txt
```

::: {.callout-note title="Delimiter tip"}
`cut` defaults to **tab**. If your file is CSV, add `-d,` (comma) and adjust field numbers accordingly.
:::

---

# File Permissions (read/write/execute)

Inspect permissions:

```bash
ls -l data/metadata.tsv
# -rw-r--r--  means: owner can read/write; group & others can read
```

Common patterns:

```bash
# Make a text file broadly readable
chmod 644 results/country_counts.txt   # u=rw, g=r, o=r

# Protect raw data (read-only for everyone)
chmod 444 data/*.csv
```

::: {.callout-tip title="Quick troubleshooting"}
- “Permission denied”? Check the directory’s permissions: `ls -ld .`  
- Need to make a **tool** executable? (Used in later scripting practical.)
  - You’ll use `chmod +x script.sh` **after** you create scripts in Practical 4.
:::

---

# Local vs HPC 

| Feature  | Laptop/Desktop | HPC Cluster |
|---|---|---|
| Access | GUI + Terminal | SSH (terminal) |
| Storage | Local disk | `/home/<user>` (small), `/proj/<project>` (shared), `/scratch` (temporary, fast) |
| Compute | 1–8 cores | Many nodes/cores via a batch scheduler |
| Data | Small/local | Large/shared — link into your project via **symlinks** |

::: {.callout-note title="Guideline on HPC paths"}
- Keep large references and reads in `/proj/<project>`  
- **Symlink** them into `~/CLI_intro/refs/`  
- Use `/scratch` for temporary I/O during jobs and clean up after
:::

::: {.callout-important title="Bonus (advanced / optional)"}
Transferring between local and HPC:

```bash
# Copy a single file
scp data/metadata.tsv your_cid@vera1.c3se.chalmers.se:/home/your_cid/CLI_intro/data/

# Sync a directory (only changes)
rsync -avh results/ your_cid@vera1.c3se.chalmers.se:/proj/course/results/

# Verify integrity on both sides (compare hashes)
md5sum data/metadata.tsv
```

This will be covered more deeply when we work **in-depth** with the HPC.
:::

---

# Guided Practice

1) **Confirm workspace & data are in the right place**

```bash
cd ~/CLI_intro
pwd
ls -1
ls -1 data
```

2) **Link a shared reference** (if provided by the course/HPC)

```bash
# Adjust the /proj path to the correct course/project location
ln -s /proj/course/shared_refs/genome.fa refs/genome.fa
ls -l refs
```

3) **Answer a question with a pipeline**

> Q: *How many samples per country are in `data/metadata.tsv`?*

```bash
cut -f2 data/metadata.tsv | sort | uniq -c | sort -nr > results/country_counts.txt
head results/country_counts.txt
```

4) **Permissions in context**

```bash
# Make results readable by all (typical for text reports)
chmod 644 results/country_counts.txt

# If you have raw data files, make them read-only to avoid accidents
chmod 444 data/*.csv   # (skip if none exist)
```

---

# Self-Check (mini-quiz)

1. In one sentence, why are **symlinks** preferred for references on HPC?  
2. What’s the difference between `>` and `>>`? Between `>` and `|`?  
3. Write a one-liner to count unique values in **column 2** of `data/metadata.tsv`.  
4. Explain why `sort` usually comes **before** `uniq`.  
5. Where should **raw data** live? Where should **generated results** be written?

---

# Summary

- Expand your **CLI_intro/** workspace from Practical 1 into a reproducible project structure.  
- Use **symlinks** to avoid copying large/shared data; keep one source of truth.  
- Build insight with **small steps** (`cut` → `sort` → `uniq`) and then combine with **pipes**.  
- Use **redirection** to save outputs in `results/`, not over your inputs.  
- Manage **permissions** to protect raw data and share read access to results.  
- These habits **transfer directly** to the HPC filesystem.
