[
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_MODULES.html",
    "href": "practicals/Practicals - Python/PRACTICAL_MODULES.html",
    "title": "Practical 4: Python Modules",
    "section": "",
    "text": "Since python is such a popular programming language, people have created thousands and thousands of packages and modules that you can download for free and make use of. The ones we mention in this course are not even the tip of this massive icebarg.\nFor example, pip contains over 675 000 packages as of the time of writing!!!\nRemember the suggested general workflow when attempting to install something:\n\ncreate new conda env\ntry conda install with conda-forge and bioconda channels. If you dont find what you need:\ninstall pip in your env and try that\nIf you still cant find the thing you need you can attempt to install from source via a github repo but that can be a little bit annoying (especially for a beginner)\n\n(You dont have to use conda if you know what youre doing. Its just a convenient suggestion. Other package managers are available)",
    "crumbs": [
      "Practical 4: Python Modules"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section",
    "href": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section",
    "title": "Practical 4: Python Modules",
    "section": "3.1 ",
    "text": "3.1 \nNavigate the imported code from premade_script.py\n\n3.1.1 \nUsing the comments in the function codons, can you describe what it does?\n\n\n3.1.2 \nWhat’s the input of the function?\n\n\n3.1.3 \nIn the main function of main.py, add a print statement to print the output of the function",
    "crumbs": [
      "Practical 4: Python Modules"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section-4",
    "href": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section-4",
    "title": "Practical 4: Python Modules",
    "section": "3.2 ",
    "text": "3.2 \nOpen the script called premade_script.py and find the function called exercise_function\n\n3.2.1 \nWhat’s the input of the function?\n\n\n3.2.2 \nWhat does this function do?\n\n\n3.2.3 \nTry calling it through main.py (see how the previous function is imported and used)",
    "crumbs": [
      "Practical 4: Python Modules"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section-8",
    "href": "practicals/Practicals - Python/PRACTICAL_MODULES.html#section-8",
    "title": "Practical 4: Python Modules",
    "section": "3.3 ",
    "text": "3.3 \nMake the exercise_function work\n\n3.3.1 \nA non standard module is needed to run the function. Which? Install/use/call it [This will depend on how we want the students to access modules. Do we go with Conda, singularity, modules, or anything else?]\n\n\n3.3.2 \nCheck the output of the script\nmain.py\n#!/usr/bin/env python3\nfrom premade_script import codons\nimport argparse\nimport os\n\n\ndef existing_file(file_path):\n    \"\"\"This function checks if the provided file path exists and is a file.\"\"\"\n    if not os.path.isfile(file_path):\n        raise argparse.ArgumentTypeError(f\"'{file_path}' is not a valid file\")\n    return file_path\n\n\ndef main():\n    # First we capture the input arguments from the command line\n    parser = argparse.ArgumentParser(description=\"Process a string and a folder.\")\n    parser.add_argument('-s', '--sequence', type=existing_file, required=True, help='Input sequence file')\n    args = parser.parse_args()\n    \n    # We first call the function from the script, and use the input string as an argument\n    sequence_codons = codons(args.sequence)\n    \n    # Here the students should check the output of the function\n    print(sequence_codons) # REMOVE THIS\n    \n    # Here the students will need to call the exercise_function from my_function.py\n    exercise_output = exercise_function(sequence_codons) # REMOVE THIS\n    \n    # And check the output of that function\n    print(exercise_output) # REMOVE THIS\nif __name__ == \"__main__\":\n    main()\npremade_script.py\ndef codons(sequence_file):\n    \"\"\"A function that reads a fasta file and returns a list of codons for each sequence in the file\"\"\"\n    \n    # First we read the fasta file and store the sequences as a string\n    sequence = \"\"\n    \n    # Open the file\n    with open(sequence_file) as fasta_file:\n        # Read the file line by line\n        for row in fasta_file:\n            # If the line starts with '&gt;', it is a header and we skip it\n            if not row.startswith('&gt;'):\n                # If the line does not start with '&gt;', it is a sequence and we add it to the list\n                sequence += row.strip()\n    \n    # Now we split the sequences into codons\n    sequence_codon_list = []\n    \n    # Loop over the sequence in steps of 3\n    for i in range(0, len(sequence), 3):\n        # Append the codon to the list if it is a full codon (3 nucleotides)\n        if i + 3 &lt;= len(sequence):\n            # Then append the codon to the list\n            sequence_codon_list.append(sequence[i:i + 3])\n    \n    # Return the list of codons\n    return sequence_codon_list\n\n\ndef exercise_function(sequence_codons):\n    \"\"\"What does this function do?\"\"\"\n    aa_string = \"\"\n    for codon in sequence_codons:\n        amino_acid = Seq(codon).translate()\n        aa_string += str(amino_acid)\n    return aa_string",
    "crumbs": [
      "Practical 4: Python Modules"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html",
    "title": "Practical 0: Introduction",
    "section": "",
    "text": "Welcome!!\nThe exercises for this first lesson are meant to connect the python part of the course to what you have previously done in the unix shell scripting part of the course and are meant to be done in the terminal (not jupyter). You can start using jupyter after today.\nIf you think there is a lot of new syntax, dont worry. We will begin explaning the python syntax from the ground up starting tomorrow.\n\n\nJupyter notebooks (text)\nJupyter notebooks (video)",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html#some-useful-links",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html#some-useful-links",
    "title": "Practical 0: Introduction",
    "section": "",
    "text": "Jupyter notebooks (text)\nJupyter notebooks (video)",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html#the-print-command-and-your-first-script",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html#the-print-command-and-your-first-script",
    "title": "Practical 0: Introduction",
    "section": "2.1 The print command and your first script",
    "text": "2.1 The print command and your first script\nTo print any string, in this case hello, you can use the following line of code in python:\nprint(\"hello!\")\n\nTry to create a script that prints a message using the command above Open a file called my_python_script.py. You can name it anything, just make sure it has the extension .py. Add a print command like the one above to print any string of text.\nSave the file and find it in the terminal. Run your script using the following command in bash:\n\npython my_python_script.py\nAny line starting with a hash # will be interpreted as a comment by python. A reasonable comment to the print command above could look something like this: # Print \"Hello\". - Try adding a comment to your script as well.",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html#check-your-python-version",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html#check-your-python-version",
    "title": "Practical 0: Introduction",
    "section": "2.2 Check your python version",
    "text": "2.2 Check your python version\nYou can check your python version by typing python -V in the terminal. Another useful argument is -h, which allows you to see what available options there are. Try it out by typing python -h in your terminal.",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html#basename",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html#basename",
    "title": "Practical 0: Introduction",
    "section": "3.1 basename",
    "text": "3.1 basename\nWhen coding we often work with file paths. However, we want to avoid hardcoding any path. Usually, we write scripts where we allow the user to set any path they want. Let’s say we’re working on a script where the user can specify their own input file path.\nStart by moving the file my_python_script.py to one of the subdirectories you created, by using os.rename():\nos.rename(\"/path/to/my_python_script.py\", \"test_dir_from_python/subdir1/my_python_script.py\")\nWe can use a function called basename to work with any path we’re given. The basename function comes from a submodule within os called path. You can get only the file name from an entire path like this:\n# Save the path to a variable called my_file_path\nmy_file_path = 'test_dir_from_python/subdir2/my_python_script.py'\n# Get only the file name\nos.path.basename(my_file_path)",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_SETUP.html#arguments",
    "href": "practicals/Practicals - Python/PRACTICAL_SETUP.html#arguments",
    "title": "Practical 0: Introduction",
    "section": "3.2 arguments",
    "text": "3.2 arguments\nBut how can we allow any user of a script to specify their own path? We allow them to specify it when running the script from the terminal, to avoid them going in and messing with the actual script.\nNow we are going to import a module called sys. Then, we will use a function from sys called argv to allow us to work with the command line argument that the user will give. Open up a new python script named input_arguments_test.py, preferably in test_dir_from_python/subdir2/, to allow us to work in the same directories. Write the following code in your file:\nimport sys\n# use the first and only argument from the user and save it as \"input_string\"\ninput_string = sys.argv[1]\n\n# print the input argument\nprint(input_string)\nSave the file and try to run it from the terminal. This time we should give an input argument, something it could print. Run it like this:\npython test_dir_from_python/subdir2/input_arguments_test.py \"This is a test string\"\nFinally, we will combine what we’ve gone through so far. Create a script with and input argument that is a path to one of your files. Make the script print the basename of the file.",
    "crumbs": [
      "Practical 0: Introduction"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FILES.html",
    "href": "practicals/Practicals - Python/PRACTICAL_FILES.html",
    "title": "Practical 5: Files",
    "section": "",
    "text": "In this exercise, you will be given a pre-made script, assignment_3.py and a FASTA file with a nucleotide sequence, assignment_3.fasta. To stay organized, make a new directory and keep all the exercise files in it.\nAt the end of this exercise, you should have two script files:\n\nAn updated version of assignment_3.py\nA second script, e.g.my_script.py",
    "crumbs": [
      "Practical 5: Files"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FILES.html#section",
    "href": "practicals/Practicals - Python/PRACTICAL_FILES.html#section",
    "title": "Practical 5: Files",
    "section": "3.1 ",
    "text": "3.1 \nThe assignment_3.py script.\n\n3.1.1 \nLook at the code in assignment_3.py. The first two functions are annotated with comments describing what they do. Go through the code to get an overview of what it actually does.\n\n\n3.1.2 \nThe last function, mystery_function, is not. Read through the code in that function and try to figure out what it does. Write a docstring (\"\"\"This thing at the beginning of a function definition\"\"\") and add comments in the same way as the first two functions, describing what this function does. If you want, you can also give it a more descriptive name.",
    "crumbs": [
      "Practical 5: Files"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FILES.html#section-3",
    "href": "practicals/Practicals - Python/PRACTICAL_FILES.html#section-3",
    "title": "Practical 5: Files",
    "section": "3.2 ",
    "text": "3.2 \nCreate a new python script (eg. my_script.py) in the same directory. In this script, import all the functions from your updated assignment_3.py. Use the functions to read assignment_3.fasta, get the first nucleotide sequence, split into codons, and finally call the mystery function on the sequence. Include the output from the mystery function as a comment in your script.",
    "crumbs": [
      "Practical 5: Files"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FILES.html#section-4",
    "href": "practicals/Practicals - Python/PRACTICAL_FILES.html#section-4",
    "title": "Practical 5: Files",
    "section": "3.3 ",
    "text": "3.3 \nIt is usually a good idea to not rely on hard-coded paths for input files. Using sys.argv or argparse, add the necessary code to get the path to the input FASTA file from the command-line.",
    "crumbs": [
      "Practical 5: Files"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html",
    "href": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html",
    "title": "Practical 1: Python Data Types and If/Else Clauses",
    "section": "",
    "text": "Now we are finally getting into the bread and butter of python syntax! Today you will do some very simple exercises with some of the most commonly used python data types as well as starting with boolean logic using if/else clauses.\nYou can copy the code blocks containing comments below and work on the tasks from within your IDE (Integrated Developer Environment)/text editor or a jupyter notebook. Remember, comments are the lines that start with # and will be ignored when you run your code\nRemember to refer back to todays presentations for help or ask the teachers - It’s what we are here for :)\n\n\nThey may even contain information that the teachers missed or didn’t explain well enough!\nData types (written help)\nData types (Video guide)\nIf/else (written guide)\nIf/else (video guide, easy)",
    "crumbs": [
      "Practical 1: Python Data Types and If/Else Clauses"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#some-links-that-you-may-find-helpful",
    "href": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#some-links-that-you-may-find-helpful",
    "title": "Practical 1: Python Data Types and If/Else Clauses",
    "section": "",
    "text": "They may even contain information that the teachers missed or didn’t explain well enough!\nData types (written help)\nData types (Video guide)\nIf/else (written guide)\nIf/else (video guide, easy)",
    "crumbs": [
      "Practical 1: Python Data Types and If/Else Clauses"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#data-type-usage",
    "href": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#data-type-usage",
    "title": "Practical 1: Python Data Types and If/Else Clauses",
    "section": "2.1 Data type usage",
    "text": "2.1 Data type usage\nHere’s some example code of how to save the number 4, which is interpreted as an integer by python, to a variable that we choose to call my_int. Then we print the variable.\nmy_int = 4\n\nprint(my_int)\nCreate a python script (.py file) where you save one value of each data type to its own variable. Afterwards, print all the variables and their type using the commands print() and type().\nData types to include: integer, floating point, string, boolean, nonetype, list, dictionary, tuple, set, range.",
    "crumbs": [
      "Practical 1: Python Data Types and If/Else Clauses"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#if-elif-else-on-your-data-types",
    "href": "practicals/Practicals - Python/PRACTICAL_DATA_TYPES_AND_IF_ELSE.html#if-elif-else-on-your-data-types",
    "title": "Practical 1: Python Data Types and If/Else Clauses",
    "section": "2.2 If / Elif / Else on your data types",
    "text": "2.2 If / Elif / Else on your data types\nFor this part you can reuse the variables you created in the previous task or recreate them with new values.\nThe variables need to be created in the same script that you use to solve the following tasks. That way, python will still “remember” what the variables are supposed to be.\n\n\n\n\n\n\nHint\n\n\n\nDouble check what type a variable is with type(variablename), to deremine which variable to use.\n\n\n\n2.2.1 \nLet’s start with strings! Below is an example code block of how to determine the length of a string:\n# Save a string to a variable\nmy_string = \"variablestuesday\"\n\n# Check how many letters are in my_string. It's the same thing as checking the length of a string.\nlen(my_string) \nPick a STRING you created. Then create an if/else-statement to determine if the string is empty or not. If the string is empty, the length should be zero.\nIf it’s empty, print “empty”, else print “non-empty”.\n\n\n2.2.2 Multi-way (If/elif/else clause)\nPick an INTEGER you created. Write an if-statement with 3 scenarios:\n\nThe integer could be positive\nThe integer could be zero\nThe integer could be negative\n\nFor every scenario, some informative text should be printed. E. g. tell the user if the integer is positive, equal to zero or negative.\n\n\n\n\n\n\nHint\n\n\n\nCompare the integer to zero.\n\n\n\n\n2.2.3 Type gate + nested classification (Nested If/elif/else)\nPick a SEQUENCE you created (list, tuple or range). Here, we want you to cerate a nested loop.\nIn the outer if-statement, check if your choice of variable matches any of the data types list or tuple or range. Use or to allow the variable to match any of them. If the variable is not a match, print “wrong type for this task”.\nThe inner if-statement should be used IF the variable was a match. Here, you need to check for 3 different scenarios:\n\nif its length is zero, print “empty”\nif the length == 1, print “single item”\nShould the length be &gt; 1, print “multiple items”",
    "crumbs": [
      "Practical 1: Python Data Types and If/Else Clauses"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "",
    "text": "Next-generation sequencing (NGS) technologies generate millions of short DNA sequences called reads. You’ve already become quite familiar with Nanopore sequencing by this point, and are aware that NGS reads are error-prone. Hence it is essential that before we do any further sequence analysis, we first assess the quality of our read data using command line tools that provide both efficiency and reproducibility.\nIn this practical, we’ll use command line tools to perform quality assessment and filtering, followed by taxonomic classification to understand what organisms are present in our samples. We will provide short-read files for you to use, as well as the code needed to perform the analyses. However, you will need to adapt paths and filenames to your own working environment, as well as select your own data and think about the biological significance of the results.\n\n\n\n\n\n\nREMEMBER: Create a separate project folder\n\n\n\n# Make this practical directory in your home directory on HPC AND your local computer\nmkdir -p ~/yourdir/practical_1\ncd ~/yourdir/practical_1\n\n# Then symlink the data directory in the n2bin-gu project dir for easy access on the HPC\nln -s  /cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq ./data\n\n# Copy the files you need to your local computer as well, but DONT overwrite the original files!\n# You can use scp or rsync for this\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq ./data\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this practical, you will be able to:\n\nUnderstand the structure and format of raw sequencing data (FASTQ files)\nUse fastp for integrated quality assessment and filtering\nInterpret quality control metrics and HTML reports\nApply quality trimming and adapter removal in a single step\nPerform taxonomic classification using kraken2 in a Singularity container",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#obtaining-sample-data",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#obtaining-sample-data",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "2.1 Obtaining sample data",
    "text": "2.1 Obtaining sample data\nFor this excersise, we offer a set of fastq file for you to Use Begin by navigating to your symlinked data directory and look at the Files\nYou wont need to copy the files to your working directory, but make sure that any edits you make dont overwrite the original files! If you want you can also do this step on your local computer!",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#examining-fastq-structure",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#examining-fastq-structure",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "2.2 Examining FASTQ structure",
    "text": "2.2 Examining FASTQ structure\n# Look at the first few reads from a compressed file\n# Use this code example for each of the sample files in the data dir\n# Either untar the files first, or use zcat to read them directly\ntar -xvzf sample1_R1.fastq.gz\ncat sample1_R1.fastq | head -8\n\n# or \nzcat sample1_R1.fastq.gz | head -8\nEach sequencing read in a FASTQ file consists of four lines:\n@read_identifier_and_description\nATCGATCGATCGATCG...\n+\n!\"#$%&'()*+,-./0...\n#########################DIVIDER#########################\n\n\n\n\n\n\nKey Questions\n\n\n\n\nWhy is the bottom line encoded? What does it represent?\nHow can you identify paired-end reads?",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#fastp-analysis",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#fastp-analysis",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "3.1 Fastp analysis",
    "text": "3.1 Fastp analysis\nThis step must be done on your local computer, make sure you download the data with scp first!\n# Begin by creating an output directory for your QC results\nmkdir -p fastp_results\n\n# Run fastp on paired-end reads with default settings\nfastp -i sample1_R1.fastq.gz -I sample1_R2.fastq.gz -o sample1_R1_filtered.fastq.gz -O sample1_R2_filtered.fastq.gz -h sample1_fastp_report.html -j sample1_fastp_report.json\n\n# Go to the output directory and check the results\n# View the HTML report in your browser \nKey metrics to examine in the HTML reports:\n\nRead quality: Before and after filtering quality distributions\nBase content: Per-position base composition\nAdapter content: Automatic adapter detection and removal\nFiltering results: Number of reads removed and why\nInsert size: The size of the DNA fragments sequenced (between the adapters and paired reads combined)\n\n\n\n\n\n\n\nKey Questions\n\n\n\n\nWhat percentage of reads were filtered out in each sample?\nWhat was the main reason for read filtering?\nHow did the quality distributions change after filtering?\nWere any adapters detected and removed?\n\n\n\nIn some cases, you may want to customize the filtering parameters, such as hard-trimming a specific number of bases from the start or end of reads, or setting a minimum read length. You can do this by adding additional flags to the fastp command. For example, a good agnostic approach is to trim to a fixed quality threshold at the 3’ end of the reads, and filter out any reads that are too short after trimming.\n# Here is an example command with additional trimming and filtering options, for a quality threshold of 30 and minimum length of 50bp\nfastp -i sample1_R1.fastq.gz -I sample1_R2.fastq.gz -o sample1_R1_filtered.fastq.gz -O sample1_R2_filtered.fastq.gz -h sample1_fastp_report.html -j sample1_fastp_report.json --cut_right --cut_mean_quality 30 --length_required 20\n\n\n\n\n\n\nKey Questions\n\n\n\n\nDid this improve or change the quality of the reads?\nWe set a minimum length of 20bp, what are the pros and cons of this choice?",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#hpc-job-script-for-kraken2-classification",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#hpc-job-script-for-kraken2-classification",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "4.1 HPC job script for kraken2 classification",
    "text": "4.1 HPC job script for kraken2 classification\nLets begin by creating the job script that we will submit to the HPC cluster. Make sure to adjust paths to your actual directories and database locations. You can either create and edit the file on the cluster, or create it locally and then upload it to the cluster if you want. ALso this jobscript will allow you to run a single sample at a time, so you will need to change the sample name for each run, or make a loop to run multiple samples if you feel up for it.\n# Create SLURM job script for kraken2 analysis\n\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J kraken2_job\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=12 # Request 1 node with 12 CPUs\n#SBATCH -t 01:00:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_1/logs/kraken2_%j.out  # Standard output\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_1/logs/kraken2_%j.err   # Standard error\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nCONTAINER_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/kraken2.sif\"\nDB_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/ref_dbs/kraken2db\"\nDATA_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/teachers/Tor/results\"\n\n# Bind paths for container\nexport SINGULARITY_BINDPATH=\"${DB_PATH}:/db,${DATA_PATH}:/data,${RESULTS_PATH}:/results\"\n\n# Create results directory\nmkdir -p ${RESULTS_PATH}\n\n# Identify sample\nsample=\"A210\"\necho \"Processing sample: ${sample}\"\n\n# Run Kraken2 classification\nsrun singularity exec ${CONTAINER_PATH} kraken2 \\\n        --db /db \\\n        --threads 8 \\\n        --paired \\\n        --output /results/${sample}_kraken2_output.txt \\\n        --report /results/${sample}_kraken2_report.txt \\\n        --classified-out /results/${sample}_classified#.fastq \\\n        --unclassified-out /results/${sample}_unclassified#.fastq \\\n        /data/${sample}_R1.fastq.gz /data/${sample}_R2.fastq.gz\n\necho \"Completed classification for ${sample}\"\n\necho \"All samples processed successfully\"",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#preparing-results-for-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#preparing-results-for-multiqc",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "5.1 Preparing results for MultiQC",
    "text": "5.1 Preparing results for MultiQC\nMultiQC works best when all results are organized in a clear directory structure. Let’s organize our outputs properly.\n# Create a comprehensive results directory\nmkdir -p multiqc_analysis/{fastp_reports,kraken2_reports}\n\n# Copy fastp outputs to the correct directory\n# Copy kraken2 outputs to the correct directory\n\n# Verify all files are present\nls -la multiqc_analysis/*/",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#running-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_RAWDATA_AND_QC.html#running-multiqc",
    "title": "Practical 1: Raw Read Data and Quality Control",
    "section": "5.2 Running MultiQC",
    "text": "5.2 Running MultiQC\n# Run MultiQC on all results to generate a comprehensive report\ncd multiqc_analysis\n\n# Generate MultiQC report with custom configuration\nmultiqc . --title \"BIO511 Practical 2 - QC and Classification Summary\" --filename \"practical2_multiqc_report\" --dirs --dirs-depth 2 --force\n\n# Open the report in your browser\n# The HTML file can be opened directly",
    "crumbs": [
      "Practical 1: Quality Control and Classification"
    ]
  },
  {
    "objectID": "practicals/README_practicals.html",
    "href": "practicals/README_practicals.html",
    "title": "BIO511 Genomics - Practical Repository",
    "section": "",
    "text": "Here you will find the practicals divided into::\n\nCommand-line programming\nBest coding practices\nPython programming\nWhole genome sequencing analysis\n\nEnjoy :)"
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "",
    "text": "This second session teaches practical collaboration with Git, step by step:\n\nResolve a merge conflict safely\nMark important states with annotated tags\nShare your work to GitHub via HTTPS or SSH keys\nUse gentle undo to fix small mistakes without panic\n\nYou should already have the ~/projects/git-lab repository from Session 1.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\nBy the end, you can: 1) Create and resolve a merge conflict by editing conflict markers\n2) Create and push annotated tags (v0.1, v0.2)\n3) Push your branch to GitHub via HTTPS, or set up SSH keys and push via SSH\n4) Use git restore and git restore --staged to undo safely",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#introduction",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#introduction",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "",
    "text": "This second session teaches practical collaboration with Git, step by step:\n\nResolve a merge conflict safely\nMark important states with annotated tags\nShare your work to GitHub via HTTPS or SSH keys\nUse gentle undo to fix small mistakes without panic\n\nYou should already have the ~/projects/git-lab repository from Session 1.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\nBy the end, you can: 1) Create and resolve a merge conflict by editing conflict markers\n2) Create and push annotated tags (v0.1, v0.2)\n3) Push your branch to GitHub via HTTPS, or set up SSH keys and push via SSH\n4) Use git restore and git restore --staged to undo safely",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#what-you-need",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#what-you-need",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "2 What You Need",
    "text": "2 What You Need\n\nGit installed (git --version)\n\nA GitHub account (free)\n\nYour git-lab repo from Session 1 (or let the commands below scaffold it)\n\n# If needed, create/enter the repo now\nmkdir -p ~/projects/git-lab && cd ~/projects/git-lab\ngit init\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\nmkdir -p config\necho -e \"project: git-lab\\nthreads: 4\\ngenome_id: ST398\" &gt; config/config.yml\ngit add config/config.yml\ngit commit -m \"Start project with config\"",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#main-concepts",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#main-concepts",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "3 Main Concepts",
    "text": "3 Main Concepts\n\nRemote: a copy of your repository hosted elsewhere (e.g., GitHub). The default name is origin.\nTracking branch: a local branch that remembers which remote branch it tracks (set with git push -u origin main).\nMerge vs fast‑forward: a merge creates a new commit that combines two histories; a fast‑forward just moves a pointer.\nConflict markers: lines Git writes inside a file to show both sides of a conflicting change — you edit the file to the final content and remove the markers.\nAnnotated tag: a named label on a commit with a message and metadata, perfect for submissions/releases.\n\n\n\n\n\n\n\nDid You Know?\n\n\n\ngit pull ≈ git fetch + git merge. Prefer git fetch first when learning, so you can inspect before merging.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#merge-conflict",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#merge-conflict",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "4 Merge Conflict",
    "text": "4 Merge Conflict\nWe’ll create a tiny conflict on one line, fix it, and finish the merge.\n1) Change on main\ngit switch -c main 2&gt;/dev/null || git switch main\necho \"genome_id: ST400\" &gt; config/config.yml\ngit add config/config.yml\ngit commit -m \"Set genome_id to ST400 on main\"\n2) Different change on a new branch\ngit switch -c feature/alt-genome\necho \"genome_id: ST401\" &gt; config/config.yml\ngit add config/config.yml\ngit commit -m \"Set genome_id to ST401 on feature branch\"\n3) Merge back into main (conflict expected)\ngit switch main\ngit merge feature/alt-genome\nOpen config/config.yml — you’ll see markers like:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ngenome_id: ST400\n=======\ngenome_id: ST401\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/alt-genome\n4) Fix the file\nDecide the final line (e.g., keep genome_id: ST401). Delete the three marker lines (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;). Save.\n5) Tell Git you fixed it and complete the merge\ngit add config/config.yml\ngit commit   # completes the merge\nCheckpoint\ngit log --oneline --decorate --graph -n 5\n\n\n\n\n\n\nCommon Mistake\n\n\n\nLeaving conflict markers in the file (the &lt;&lt;&lt;&lt;&lt;&lt;&lt; ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; lines) will confuse you later. Remove them all before committing.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#tag-important-states",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#tag-important-states",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "5 Tag Important States",
    "text": "5 Tag Important States\nThink of a tag as a sticky note on a specific snapshot.\ngit tag -a v0.1 -m \"First tagged version after conflict resolution\"\ngit tag          # should list v0.1\ngit show v0.1    # see what it points to\n\n\n\n\n\n\nBest Practice\n\n\n\nUse annotated tags (-a) with a short message to mark submissions, milestones, or results you cite.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#share-to-github-https-or-ssh",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#share-to-github-https-or-ssh",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "6 Share to GitHub — HTTPS or SSH",
    "text": "6 Share to GitHub — HTTPS or SSH\n\n6.1 Option 1 — HTTPS\n\nCreate an empty GitHub repo named git-lab (do not add a README).\n\nConnect your local repo (replace YOURUSER):\n\ngit remote add origin https://github.com/YOURUSER/git-lab.git\ngit remote -v\n\nPush main and set tracking (you’ll sign in or use a token the first time):\n\ngit push -u origin main\n\nPush your tag(s):\n\ngit push --tags\n\n\n\n\n\n\nCommon Mistake\n\n\n\nIf you accidentally created the GitHub repo with a README, run git pull once before your first push.\n\n\n\n\n6.2 Option 2 — SSH (BONUS SECTION)\nThis is a one‑time setup per machine. Use it if you prefer entering a passphrase once and pushing without tokens.\nStep 1 — Do you already have a key?\nls -al ~/.ssh\n# If you see id_ed25519 and id_ed25519.pub, you likely already have a key.\nStep 2 — Create a key (if needed)\nssh-keygen -t ed25519 -C \"you@example.com\"\n# Press Enter to accept the default location (~/.ssh/id_ed25519)\n# Choose a passphrase (recommended) or press Enter for none\nStep 3 — Add the key to your ssh-agent\n# Start the agent (if not running) and add your key\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\nStep 4 — Copy your PUBLIC key and add it on GitHub\n# macOS: copy to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\n# If pbcopy isn't available, print and copy manually:\ncat ~/.ssh/id_ed25519.pub\nOn GitHub: Settings → SSH and GPG keys → New SSH key → paste → Save.\nStep 5 — Test your SSH connection\nssh -T git@github.com\n# Expect a greeting like: \"Hi &lt;username&gt;! You've successfully authenticated...\"\nStep 6 — Switch your remote from HTTPS to SSH and push\ngit remote set-url origin git@github.com:YOURUSER/git-lab.git\ngit remote -v\ngit push -u origin main\ngit push --tags\n\n\n\n\n\n\nCommon Mistakes\n\n\n\n\nDon’t paste your private key; paste the .pub file only.\n\n“Repository not found”? Check the username/repo and that it exists.\n\nLocked‑down networks may block SSH — switch back to HTTPS if needed: git remote set-url origin https://github.com/YOURUSER/git-lab.git",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#undoing",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#undoing",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "7 Undoing",
    "text": "7 Undoing\n\nUnstage a file (keep changes):\ngit restore --staged &lt;file&gt;\n\nDiscard local edits in a file (careful):\ngit restore &lt;file&gt;",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#collaboration-etiquette",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#collaboration-etiquette",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "8 Collaboration Etiquette",
    "text": "8 Collaboration Etiquette\n\nKeep branches short‑lived and focused.\n\nWrite imperative commit subjects (e.g., “Add README note”).\n\nPush early; open a short Pull Request describing the change in one sentence.\n\nTag milestones and note them in your README.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#guided-practice",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#guided-practice",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "9 Guided Practice",
    "text": "9 Guided Practice\n\nConflict practice — Make a new branch feature/readme-title, edit the same line in README.md on main and the branch, merge, resolve markers, commit.\n\nTag & explain — Create v0.2 on your current main. Run git show v0.2 and add a one‑line note to README.md about what v0.2 represents. Commit.\n\nRemote sync — Push main, push v0.2. If you used HTTPS, try switching to SSH and push again.\n\nUndo drill — Stage a file by mistake, then unstage it with git restore --staged &lt;file&gt;. Make a test edit and discard it with git restore &lt;file&gt;.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#troubleshooting-faq",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#troubleshooting-faq",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "10 Troubleshooting & FAQ",
    "text": "10 Troubleshooting & FAQ\n\n“Permission denied (publickey)” → Add your public key to GitHub and test with ssh -T git@github.com.\n\n“Repository not found” → Check the remote URL and your permissions; ensure the repo exists.\n\n“Updates were rejected” → The remote has work you don’t. Run git fetch, inspect with git log --oneline --decorate --graph --all, then git merge or git pull.\n\nAccidentally tagged the wrong commit → Delete the local tag with git tag -d v0.1, delete it on the remote with git push --delete origin v0.1, re‑create on the correct commit, push tags again.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#summary",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#summary",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "11 Summary",
    "text": "11 Summary\nToday you practiced real collaboration moves: - Resolved a merge conflict - Created and pushed annotated tags - Shared your repo to GitHub via HTTPS or SSH - Used undoing to safely back out of small mistakes\nYou’re now ready to participate in simple team workflows and submit reproducible snapshots of your work.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#cheat-sheet",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION2.html#cheat-sheet",
    "title": "Git Session 2 — Conflicts, Tags, Remotes (HTTPS & SSH), and Undo",
    "section": "12 Cheat Sheet",
    "text": "12 Cheat Sheet\nConflict:   git switch main → git merge &lt;branch&gt; → edit file → git add → git commit\nTags:       git tag -a &lt;name&gt; -m \"msg\"  |  git show &lt;name&gt;  |  git push --tags\nHTTPS:      git remote add origin https://github.com/USER/REPO.git  |  git push -u origin main\nSSH setup:  ssh-keygen -t ed25519 -C \"you@ex\"; pbcopy &lt; ~/.ssh/id_ed25519.pub; add key on GitHub\nSSH push:   git remote set-url origin git@github.com:USER/REPO.git  |  git push -u origin main\nUndo:       git restore --staged &lt;file&gt;   |   git restore &lt;file&gt;",
    "crumbs": [
      "Coding Best Practices",
      "Practical 2: Git Fundamentals: Session 2"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html",
    "title": "Practical 5: Software Installation and Containers",
    "section": "",
    "text": "Up until now youve had to make everything by yourself (sort of, at least using base programs that come with your unix distro). This approach, while flexible, defeats one of they key benefits of modern bioinformatics; Open source and collaborative solutions to large data handling. Thanks to a lot of effort, there are countless of free, open-source tools available in various repositiories on the internet.\nHowever these tools, while saving us a lot of time, may be tricky to find and set up correctly. Different tools require different dependencies, versions, or system libraries. This often leads to complex webs of libraries and packages that act as the backbone of your code, which can be difficult to manage and reproduce on other systems. In this practical, you’ll learn where to find tools, various ways of installing an managing them (conda, apt), cloning repos from GitHub, and—most importantly—how to package tools in a Singularity/Apptainer containers that you can copy to, and run, anywhere (including HPC environments).\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this practical you should be able to:\n\nFind and identify bioinformatics tools in public repositories\nInstall software locally using package managers (apt, conda)\nCreate and manage conda environments for isolated software setups\nWrite a basic Singularity definition file to create a container from a base image\nBuild and test a Singularity container locally\nTransfer and run a Singularity container on an HPC using SLURM\n\n\n\n\n\n\n\n\n\n\nREMEMBER: Create a separate project folder\n\n\n\nmkdir -p ~/yourdir/practical_5\ncd ~/yourdir/practical_5",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#exercises-discover-tools",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#exercises-discover-tools",
    "title": "Practical 5: Software Installation and Containers",
    "section": "2.1 Exercises: Discover tools",
    "text": "2.1 Exercises: Discover tools\n# Explore apt \n# Find which apt subcommand lists packages for the following queries:\napt search kraken2\napt search singularity\napt search prokka\n\n# Lets look a little closer at prokka:\napt show prokka\n\n# Explore a tool on GitHub:\n#   1) Find the repository for 'BAKTA'\n#   2) Locate the installation methods, what options are available?\n#   3) Identify dependencies and OS requirements",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#setting-up-conda",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#setting-up-conda",
    "title": "Practical 5: Software Installation and Containers",
    "section": "3.1 Setting up Conda",
    "text": "3.1 Setting up Conda\nTo begin with, we will install Miniconda, a minimal installer for Conda. Conda is an open-source package management system and environment management system that runs on Windows, macOS, and Linux. It quickly installs, runs, and updates packages and their dependencies. Conda also creates, saves, loads, and switches between environments on your local computer. This is particularly useful in bioinformatics, where different projects may require different versions of software and libraries.\n# Install Miniconda\n# For this we need to fetch files from the internet via wget \nwget -O Miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda.sh -b -p $HOME/miniconda3\n\n# Initialize shell and set channels\neval \"$($HOME/miniconda3/bin/conda shell.bash hook)\"\nconda init\n\n# Open a new shell terminal\n\n# Configure channels for bioconda packages, a specialized repository for bioinformatics software\nconda config --add channels conda-forge\nconda config --add channels bioconda\nconda config --set channel_priority strict",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#install-fastp-locally-via-apt",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#install-fastp-locally-via-apt",
    "title": "Practical 5: Software Installation and Containers",
    "section": "3.2 Install fastp locally via apt",
    "text": "3.2 Install fastp locally via apt\nNext up, we will install fastp locally using apt, the default package manager for Debian-based Linux distributions. You will encounter fastp later in the genomics workflow praticals, its a tool inteded for quality control and preprocessing of FASTQ read files.\n# Before we install anything via apt, its always a good idea to update the package lists\n# This ensures you get the latest versions available in the repositories\nsudo apt update\n\n# Search for fastp package in apt repositories\napt search fastp\n\n# Install fastp\nsudo apt install -y fastp\n\n# Verify installation\nfastp --version",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#use-conda-to-install-multiqc-locally",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#use-conda-to-install-multiqc-locally",
    "title": "Practical 5: Software Installation and Containers",
    "section": "3.3 Use Conda to install MultiQC locally",
    "text": "3.3 Use Conda to install MultiQC locally\nFor this segment we will install MultiQC, a tool that aggregates results from bioinformatics analyses across many samples into a single report. This is particularly useful for summarizing the output of tools like FastQC. Our install will be done in a conda enviroment, installing the version hosted on bioconda.\n# Create a new conda environment named MultiQC with python 3.9\nconda create -n MultiQC python=3.9\n\n# Activate the environment\nconda activate MultiQC\n\n# Install MultiQC from bioconda\nconda install multiqc\n\n# Verify installation\nmultiqc --version",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#containerization-with-singularity",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#containerization-with-singularity",
    "title": "Practical 5: Software Installation and Containers",
    "section": "4.1 Containerization with Singularity",
    "text": "4.1 Containerization with Singularity\nContainers are a way to package software and its dependencies into a single, portable unit that can run consistently across different computing environments. Unlike virtual machines, containers share the host system’s kernel, making them more lightweight and efficient. This is particularly useful in bioinformatics, where software often has complex dependencies and needs to run on various systems, including high-performance computing (HPC) clusters.\nOn HPC you usually cannot sudo and may not be allowed to build images on the login/compute nodes. Best practice is then: - Build the image locally (or via a remote builder). - Copy the .sif to the HPC. - Bind-mount data and databases at runtime.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#a-minimal-kraken2-singularity-definition-file",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#a-minimal-kraken2-singularity-definition-file",
    "title": "Practical 5: Software Installation and Containers",
    "section": "4.2 A minimal kraken2 Singularity definition file",
    "text": "4.2 A minimal kraken2 Singularity definition file\nThe first step to creating a singularity container is to write a definition file. This file describes the base image, metadata, and the steps needed to install and configure the software inside the container. In many cases it will be simple, if an already hosted image exists (e.g. on BioContainers), luckily kraken2 is one of those cases. if you want to learn more about writing definition files, check out the Singularity documentation.\nCreate a file named kraken2.def with the contents below. It uses the BioContainers kraken2 image as a base.\n# kraken2.def\nBootstrap: docker\nFrom: quay.io/biocontainers/kraken2:2.1.6--pl5321h077b44d_0\n\n%labels\n    maintainer Y/N\n    org.opencontainers.image.title \"kraken2\"\n    org.opencontainers.image.source \"https://github.com/DerrickWood/kraken2\"\n\n%help\n    # Help exists to provide users with information about the container.\n    # Running `singularity run-help kraken2.sif` will display this message.\n    \n    Kraken 2 in a Singularity/Apptainer container.\n    \n    Bind your database at runtime, e.g. --bind /path/to/db:/db and use --db /db.\n\n    Used for the BIO511 Genomics course at Univsersity of Gothenburg.\n\n%post\n  # Here is where you would install additional software if needed and make config changes.\n  # However, the BioContainers image already has kraken2 installed.\n\n  # But for the hell of it, put a little text file in /home\n    echo \"This is a kraken2 container built from BioContainers base image.\" &gt; /home/kraken2_info.txt\n\n    # Echo a message to indicate successful build\n    echo \"kraken2 installed. Use --bind to mount a database at runtime.\"\n\n%environment\n    # Here is where you would usually set environment variables.\n    # However, the BioContainers image already has kraken2 in PATH. so nothing needed here.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#build-test-and-run-locally",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#build-test-and-run-locally",
    "title": "Practical 5: Software Installation and Containers",
    "section": "4.3 Build, test, and run locally",
    "text": "4.3 Build, test, and run locally\n# Build a read-only image from the def file (locally)\nsingularity build kraken2.sif kraken2.def\n\n# Test that kraken works by running a version check\nsingularity exec kraken2.sif kraken2 --version\n\n# We also made a little text file in /home, lets check that too\n# For this, try to shell into the container and once your in try to print the contents of the file to stdout\nsingularity shell kraken2.sif",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#slurm-job-script-to-invoke-singularity-container-based-software",
    "href": "practicals/Practicals - CLI/PRACTICAL_SOFTWARE_INSTALLATION_AND_CONTAINERS.html#slurm-job-script-to-invoke-singularity-container-based-software",
    "title": "Practical 5: Software Installation and Containers",
    "section": "5.1 SLURM job script to invoke singularity container based software",
    "text": "5.1 SLURM job script to invoke singularity container based software\nAs you already know, running software in the login node of an HPC is frowned upon. Instead, you should submit jobs to the scheduler (SLURM in this case), which allows you to take up space on one of the production nodes in the cluster. Below is a example of a SLURM job script that i (Tor) have used to run a Roary pangenome analysis on another cluster. Se if you can configure it to run kraken2 instead. All HPCs are different, so make sure to adjust paths, partitions, memory, time, etc to fit your HPC. The manual for Vera can be found here.\n#!/bin/bash -l\n#SBATCH -A nbin2-gu               # Project ID\n#SBATCH -J Roary_Pangenome        # Job name\n#SBATCH -t 20:00:00               # Time limit \n#SBATCH -n 1                      # Number of tasks (processes)\n#SBATCH -p main                   # Partition to submit to\n#SBATCH --mem=200G                # Memory per node\n#SBATCH --output=/cfs/klemming/projects/supr/naiss2024-22-639/05_PANGENOME_ANALYSIS/logs/roary_%j.out  # Standard output\n#SBATCH --error=/cfs/klemming/projects/supr/naiss2024-22-639/05_PANGENOME_ANALYSIS/logs/roary_%j.err   # Standard error\n\n# Load necessary modules (Do you need to load any modules for singularity on your HPC?)\nml singularity/4.1.1-cpeGNU-23.12\n\n# Define paths to script and container\nSINGULARITY_CONTAINER=\"/cfs/klemming/projects/supr/naiss2024-22-639/DBGWAS_test\"\"\n\n# Execute script within singularity environment\nsrun singularity exec -B /cfs/klemming/projects/supr/nbin2-gu:/mnt/data $SINGULARITY_CONTAINER roary -e --mafft -p 8 -f /mnt/data/05_PANGENOME_ANALYSIS/output/ /mnt/data/05_PANGENOME_ANALYSIS/input/*.gff\nOnce a jobscript is done, you can submit it to SLURM with the sbatch command, monitor its progress with squeue, and check the output files once its done.\nWe wont be running a jobscript for kraken2 here, but try to set one up yourself. You can use the jobscript above as a template Well run kraken2 for real in the next practical\n# Example of submitting a jobscript\nsbatch job_kraken2.slurm\n\n# Monitor\nsqueue -u $USER\n# or \nscontrol show job JOBID\n\n# Check output files\ntail -20 kraken2_12345.out\n\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nPackage managers (apt, conda) simplify software installation and dependency management.\nConda environments allow for isolated setups, preventing conflicts between projects.\nContainers (Singularity/Apptainer) provide portability and reproducibility across different systems.\nAlways bind necessary directories (data, databases) when running containers on HPC.\n\n\n\n\n\n\n\n\n\nPro Tips\n\n\n\n\nTest containers locally before HPC deployment. Run exec, shell, and run commands to ensure functionality.\nKeep containers minimal; BioContainers images are a quick and reliable base.\nUse %help and labels in your definition file; inspect with singularity run-help and singularity inspect. This helps reproducibility for other users.\nBind data with --bind host_dir:container_dir; pass multiple directories with variables\nIn some cases, you may need additional config for your runs. Instead of making complex invokes in the jobscript, consider writing a script to run the analysis, and then invoke that script within the container.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 5: Software & Containers"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "",
    "text": "Now that you’ve learned the basics of the command-line, its high time that we start working on making our coding structured and reproducible. While runnning pipes in our command line prompt can be quick and easy, writing our code in structured machine-readable text files called scripts, allows us to expand the possibilities of our programming. In genomics, you’ll pretty much always need to process massive datasets - think millions of sequences, GWAS data with millions of variants, or annotation files with complex formatting. Typically these files have a standardized structure, which the handling of we can automate by creating generalizable and well-formatted scripts.\n\n\n\n\n\n\nREMEMBER: CREATE A SEPARATE PROJECT FOLDER IN YOUR HOME DIRECTORY FOR THIS PRACTICAL\n\n\n\nmkdir -p ~/yourdir/practical_4\ncd ~/yourdir/practical_4\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this practical you should be able to:\n\nUse awk to execute short programs for text manipulation\nWrite iterative bash scripts using for loops and if/else statements\nDebug your scripts using global set-options, traps and custom error messages\nBe able to write basic flexible shell scripts using OPTARGS",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#download-data",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#download-data",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "2.1 Download data",
    "text": "2.1 Download data\nFor this practical, we will work with a set of five E. coli genomes. You may either retrieve these from the canvas course page, or if you want, you can try to fetch them from the HPC group directory (if you have access).\n# Download the data from canvas\n# or \n# use scp (secure copy) to copy the data from the HPC group directory\nscp &lt;CIDusername&gt;@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/practical4/DATAPRACTICAL4.tar.gz ~/CLI_intro/practical_4/data/\n\n# Unpack the data\ntar -xvzf ~/CLI_intro/practical_4/data/DATAPRACTICAL4.tar.gz -C ~/CLI_intro/practical_4/data/",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#awk-pattern-scanning-and-processing-language",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#awk-pattern-scanning-and-processing-language",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "2.2 awk: Pattern Scanning and Processing Language",
    "text": "2.2 awk: Pattern Scanning and Processing Language\nawk can do things that may at first glance remind us of other commands like grep, since we can similarly search for patterns in text files and output them into the stdout. However, awk is much more powerful text manipulation tool that allows us to do advanced language processing and data extraction not to dissimilar to a programming language. Using awk we can accomplish short programming tasks directly in the command line.\n\nawk (Aho, Weinberger, and Kernighan): Allows for advanced text processing and file manipulation\n\nSome key commands:\n\n-F: Specifies the input field separator (default is whitespace)\nBEGIN{}: Block of programming that runs before processing any input lines (before text is put into $1, $2, etc)\nEND{}: Block that runs after processing all input lines (after all text has been processed)\n{}: Block that runs for each input line (used for processing)\n$1, $2, ...: Represents the first, second, etc. fields (eg. columns) in the current line\nOFS: Output field separator (default is a space, can be set in BEGIN block)\n\nExercises\n# awk is great for quick column selection and simple filtering\n# The syntax is 'awk option 'program' input-file' where program is a series of commands\n# Print seqid and type columns of a gff file\nawk -F'\\t' 'BEGIN{OFS=\"\\t\"} {print $1,$3,$9}' AF316.gff &gt; seqid_type.tsv\n\n# You can also filter rows based on conditions, to retain only specific features (similar to what we can accomplish with grep)\n# Here we can filter for only CDS features\nawk -F'\\t' '$3==\"CDS\"' AF316.gff &gt; cds_features.tsv\n\n# Another thing is performing operations on the data in transit, similar to what would need a pipe and multiple commands otherwise\n# Here we can calculate the length of each CDS feature and output it as a new column\n# Consider which columns should be used to calculate the length\nawk -F'\\t' 'BEGIN{OFS=\"\\t\"} $3==\"CDS\"{print $1,$4,$5,$9,($5-$4+1)}' AF316.gff &gt; cds_features_length.tsv\n\n# Lets operate some more on the feature length file\n# Calculate the total number of CDS features and their average length\nawk -F'\\t' 'BEGIN{OFS=\"\\t\"} {total_length+=($5-$4+1); count++} END{print \"Total_CDS\",count,\"Average_Length\",total_length/count}' cds_features_length.tsv\nAwk programs can get quite complex, and we will not cover all its features here. However, it is a very useful tool to have in your toolbox, and I encourage you to explore it further on your own. A good resource is the GNU Awk User’s Guide.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#starting-a-bash-script",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#starting-a-bash-script",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "3.1 Starting a bash script",
    "text": "3.1 Starting a bash script\nWe are already familiar with bash, one of the most common shell used in Linux and macOS, and is the default shell on most HPC systems. A bash script is simply a text file containing a series of bash commands that can be executed in sequence.\nExercises: Starting a script\n# For this practical, we will write a script that processes the AF genome files and looks for genes of a certain type\n# Start by creating a new file called process_AF_genome.sh\n# You can do this either by creating one manually in your vscode editor, or by using the touch command\ntouch process_AF_genome.sh\n\n# Open the file in your editor and add the shebang line at the top\n# A shebang line tells the system which interpreter to use to run the script, an interpreter is the program that will read and execute the script\n# Add this line at the top of your script:\n#!/usr/bin/env bash\n\n# Now change the file permissions to make it executable\nchmod +x process_AF_genome.sh\n\n# To run the script, simply run the filename in your terminal\n./process_AF_genome.sh",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#variables-for-loops-and-ifelse-statements",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#variables-for-loops-and-ifelse-statements",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "3.2 Variables, for loops and if/else statements",
    "text": "3.2 Variables, for loops and if/else statements\nOur next step is to add some functionality to our script. We will start by adding variables ${VAR_NAME}, these variables can be used to store values that we can use later in the script. This is useful for storing file names, parameters, or any other values that we want to use multiple times in the script.\nExercises: Variables\n# Good variables to start with are input and output file names\n# For these variables, we typically use the full path to the file, this makes it easier to run the script from any directory\n# Swap the paths below with the correct paths to your files\nINPUT_DIR=\"path/to/data\"\nOUTPUT_DIR=\"path/to/output\"\n\n# You can also use variables to store parameters, such as the gene name pattern we want to look for in our ffn files\nGENE_NAME_PATTERN=\"gene_name\"\n\n# You can also Initialize variables to store intermediate values, such as the total number of genes matching the pattern found\nTOTAL_GENES_FOR_PATTERN=0\nBut the ffn files we have contain a multitude of gene sequences and their headers, extracting these genes across multiple files will require repeated parsing setups. This is where for loops come in, they allow us to iterate over a list of items and execute a block of code for each item in the list. In our case, we can use a for loop to iterate over a list of input files and process each file in turn.\n\n* (wildcard): A character that can be used to match any string of characters in file names or patterns\nfor: A control flow statement that allows us to iterate over a list of items and execute a block of code for each item\ndo and done: Keywords used to define the start and end of a loop block\nbasename: A command that extracts the base name of a file (the file name without the path and extension)\n~: represents the matching operator, used to check if a string matches a pattern\n\nExercises: For loops\n# Also add a creation of the counts file\necho -e \"Genome\\tGene_Count\" &gt; ${OUTPUT_DIR}/gene_counts.tsv\n\n# In our INPUT_DIR we have multiple ffn files, a good way (if you have a good structured folder) to iterate over all ffn files is to use a for loop with a wildcard\nfor ffn_file in ${INPUT_DIR}/*.ffn; do \n  # putting the wilcard alongside the file extension will match all files with that extension\n  # Inside the loop, we can use the variable ffn_file established in the loop to refer to the current file being processed\n  # Extract the base name of the file (without the path and extension) to use in the output file name\n  base_name=$(basename ${ffn_file} .ffn)\n  \n  # Use awk to extract gene sequences matching the GENE_NAME_PATTERN and save them to an output file\n  # Since we are working on a fasta file, tab-separation is not needed and all data is in one column\n  awk -v pattern=\"${GENE_NAME_PATTERN}\" 'BEGIN{OFS=\"\\t\"} /^&gt;/{if($0 ~ pattern) {print $0; getline; print $0}}' ${ffn_file} &gt; ${OUTPUT_DIR}/${base_name}_filtered.ffn\n\n  # Count the number of genes found and add it to the total\n  gene_count=$(grep -c '^&gt;' \"${OUTPUT_DIR}/${base_name}_filtered.ffn\" || true)\n  TOTAL_GENES_FOR_PATTERN=$((TOTAL_GENES_FOR_PATTERN + gene_count))\n\n  # Also store the gene count for the current file in a new file for later use \n  echo -e \"${base_name}\\t${gene_count}\" &gt;&gt; ${OUTPUT_DIR}/gene_counts.tsv\n  \n  # Echo the number of genes found in the current file to you stdout, to report progress\n  echo \"Processed ${ffn_file}, found ${gene_count} genes matching pattern '${GENE_NAME_PATTERN}'\"\ndone\nLastly, we want to add some conditional logic to our script, so that we can handle different scenarios based on the input data. This is where if/else statements come in, they allow us to execute a block of code if a certain condition is met, and another block of code if the condition is not met. We’ll also introduce one additional loop to do iterative operation on the lines of a file.\n\nif: A control flow statement that allows us to execute a block of code if a certain condition is met\nthen: A keyword used to define the start of an if block\nelse: A keyword used to define an alternative block of code if the condition is not met\nfi: A keyword used to define the end of an if block\nwhile: A control flow statement that allows us to execute a block of code repeatedly as long as a specified condition is true\nread: A command that reads a line of input from a file or stdin\n-gt: A comparison operator that means “greater than”\n\nExercises: If/else\n# Conditional logic can be used in multiple ways, we will focus on an example where label our genomes based on the outcome of the for loop \n# To accomplish this, we will read the gene_counts.tsv file line by line and use an if/else statement to label the genomes based on the gene count\n# First, create a new file to store the metadata with labels\necho -e \"Genome\\tGene_Count\\tLabel\" &gt; \"${OUTPUT_DIR}/genomes_metadata_labeled.tsv\"\n\n# Now we can add some conditional logic to label our genomes\nwhile read -r line; do\n  # skip the header line\n  if [[ \"$line\" == Genome* ]]; then\n    continue\n  fi\n\n  # Read the genome name and gene count from the file\n  genome_name=$(echo \"$line\" | awk -F'\\t' '{print $1}')\n  gene_count=$(echo \"$line\" | awk -F'\\t' '{print $2}')\n\n  # Use an if/else statement to label the genome based on the gene count\n  if [ \"${gene_count}\" -gt 0 ]; then\n    label=\"label1\"\n  else\n    label=\"label2\"\n  fi\n\n  # Append the label to the metadata file\n  echo -e \"${genome_name}\\t${gene_count}\\t${label}\" &gt;&gt; \"${OUTPUT_DIR}/genomes_metadata_labeled.tsv\"\ndone &lt; \"${OUTPUT_DIR}/gene_counts.tsv\"",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#debugging-bash-scripts-when-things-go-wrong",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#debugging-bash-scripts-when-things-go-wrong",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "4.1 Debugging Bash Scripts: When Things Go Wrong",
    "text": "4.1 Debugging Bash Scripts: When Things Go Wrong\nBash scripts differ a bit from other programming languages (or atleast in the context you learn it, think CLI vs Spyder), in that they without configuration will continue running even if an error occurs. This can be both a blessing and a curse, as it allows for scripts to continue running even if a minor error occurs, but it can also make it difficult to identify and fix errors when they do occur. Finding and catching these errors is the other half of programming, we will now introduce a few tools that can help you debug your bash scripts.\nExercises: Global script settings\n# At the top your script we typically add set commands to define general behaviour for our script\n# These commands will most likely always be in your scripts\nset -e  # Exit immediately if a command exits with a non-zero status\nset -u  # Treat unset variables as an error when substituting\nset -o pipefail  # Return the exit status of the last command in the pipe that failed\n\n# One additional setting is to add debugging output\n# This will print each command before it is executed, which can be useful for debugging\n# But it can also be very verbose, and clog up your output/logs\nset -x  # Enable debugging output\nThese settings can be added at the top of your script, after the shebang line (#!/usr/bin/env bash) (also add the shebang line at the top!). However, they wont help with finding the exact spot where the error occurs. One way to debug your code easily, is to use echo and create custom error messages at different stages of your script. Another way is to use traps, however we will not cover that here.\nExercises: Error messages\n# This a quite common use of if/else statements for bash scripts\n# Here is an example of a custom error message\nif [ ! -f \"$INPUT_FILE\" ]; then\n  echo \"Error: Input file $INPUT_FILE does not exist.\"\n  exit 1\nfi\nGo ahead and identify stages in the script from before where you think errors might occur, and add custom error messages where appropriate. Remember to test your script with both valid and invalid inputs to see how it behaves\n# Here are some suggestions for custom error messages: \n# You dont need to all of these if you dont want to, but if you do, make sure they are placed at the correct spots in the script\n# Check if input and output directories exist\nif [ ! -d \"$INPUT_DIR\" ]; then\n  echo \"Error: Input directory $INPUT_DIR does not exist.\"\n  exit 1\nfi\n# and \nif [ ! -d \"$OUTPUT_DIR\" ]; then\n  echo \"Error: Output directory $OUTPUT_DIR does not exist.\"\n  exit 1\nfi\n\n# Check if a name pattern is provided\nif [ -z \"$GENE_NAME_PATTERN\" ]; then\n  echo \"Error: Gene name pattern is not provided.\"\n  exit 1\nfi\n\n# Print the basename of the file being processed in the for loop\n# allows you to later check that all files were processed\necho \"Processing file: ${base_name}\"\n\n# Check if any genes were found in the current file\nif [ \"${gene_count}\" -eq 0 ]; then\n  echo \"Warning: No genes matching pattern '${GENE_NAME_PATTERN}' found in ${ffn_file}\"\nfi\n\n# At the end of the script, print the total number of genes found\necho \"Total genes matching pattern '${GENE_NAME_PATTERN}': ${TOTAL_GENES_FOR_PATTERN}\"\n\n# For the while loop, check if the gene_counts.tsv file is not empty before processing\nif [ ! -s \"${OUTPUT_DIR}/gene_counts.tsv\" ]; then\n  echo \"Error: gene_counts.tsv file is empty or does not exist.\"\n  exit 1\nfi",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#optargs---adding-flags-to-make-your-scripts-flexible-this-is-optional",
    "href": "practicals/Practicals - CLI/PRACTICAL_SCRIPTS_AND_ADVANCED_TEXT_MANIPULATION.html#optargs---adding-flags-to-make-your-scripts-flexible-this-is-optional",
    "title": "Practical 4: Scripts and Advanced Text Manipulation",
    "section": "4.2 OPTARGS - Adding flags to make your scripts flexible (THIS IS OPTIONAL)",
    "text": "4.2 OPTARGS - Adding flags to make your scripts flexible (THIS IS OPTIONAL)\nUp until now, our scripts have been quite rigid - they expect specific input files and parameters; an occasionally bad habit that coders refer to as “hardcoding”. However, in many scenarios, we often want our scripts to be more flexible and adaptable to different inputs and options. This is where OPTARGS comes in - a built-in bash tool that allows us to easily handle command-line options and arguments. You might recognize this from the command-line tools we’ve used so far, where we can add flags like -h for help or -v for verbose output. By using OPTARGS in our scripts, we can create flags for our own programs, making them more user-friendly and versatile.\nA few key commands that we will use in this section:\n\ngetopts: A built-in bash tool for parsing command-line options and arguments\ncase statement: A control flow statement that allows us to execute different blocks of code based on the value of a variable\n\nExercises\n# To begin using OPTARGS, we need to add a while loop at the top of our script to handle the options\n# the OPTARG variable will hold the value of whatever comes after the flag in the command line\n# A while loop iterates over the options provided to the script, and sets option variables based with the read value from the command line\nwhile getopts \"i:o:p:h\" opt; do\n  case $opt in\n    i) INPUT_DIR=\"$OPTARG\" ;;  # -i flag for input directory\n    o) OUTPUT_DIR=\"$OPTARG\" ;; # -o flag for output directory\n    p) GENE_NAME_PATTERN=\"$OPTARG\" ;; # -p flag for gene name pattern\n    h) echo \"Usage: $0 -i input_dir -o output_dir -p gene_name_pattern\"  # -h flag for help\n       exit 0 ;;\n    *) echo \"Invalid option: -$OPTARG\" &gt;&2\n       exit 1 ;;\n  esac\ndone\n\n# We can add checks to ensure that the required options are provided\nif [ -z \"$INPUT_DIR\" ] || [ -z \"$OUTPUT_DIR\" ]; then\n  echo \"Error: Input and output directories are required.\"\n  echo \"Usage: $0 -i input_dir -o output_dir -p gene_name_pattern\"\n  exit 1\nfi\n\n# Its also a good idea to set default values for optional parameters, you do this in the same way as when you set variables normally\nGENE_NAME_PATTERN=\"${GENE_NAME_PATTERN:-gene_name}\"  # Default pattern if not provided\n# This sets GENE_NAME_PATTERN to the value provided by the -p flag, or to \"gene_name\" if no value is provided (as we did before)\nNow take the script you wrote before, and modify it to use OPTARGS for input and output files as well as a pattern selection OPTARGS should be placed at the top of your script, after the shebang line and global settings Remember to test your script with different combinations of options and arguments to ensure it behaves as expected",
    "crumbs": [
      "Intro to Command Line",
      "Practical 4: Scripting and Advanced Text Manipulation"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html",
    "title": "Best Coding Practices for Command‑Line Programming",
    "section": "",
    "text": "This short guide introduces habits for writing reliable command‑line (Bash) scripts.\nWhy it matters - Scripts don’t crash silently. - You understand what the script did later in future too. - Results are reproducible and don’t get overwritten by accident."
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#introduction",
    "href": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#introduction",
    "title": "Best Coding Practices for Command‑Line Programming",
    "section": "",
    "text": "This short guide introduces habits for writing reliable command‑line (Bash) scripts.\nWhy it matters - Scripts don’t crash silently. - You understand what the script did later in future too. - Results are reproducible and don’t get overwritten by accident."
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#the-six-golden-rules",
    "href": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#the-six-golden-rules",
    "title": "Best Coding Practices for Command‑Line Programming",
    "section": "2 The Six Golden Rules",
    "text": "2 The Six Golden Rules\n\nStart every Bash script safely\nUse strict mode so mistakes fail fast.\n#!/usr/bin/env bash\nset -euo pipefail       # exit on error, unset var is error, fail on pipe errors\nIFS=$'\\n\\t'              # safer word splitting\nAlways offer help\nLet users see how to run the script.\nusage() {\n  echo \"Usage: $(basename \"$0\") -i INPUT -o OUTDIR [--force]\"\n  echo \"Example: $(basename \"$0\") -i data/sample.txt -o results\"\n}\n# Show help if asked\n[[ ${1:-} == \"-h\" || ${1:-} == \"--help\" ]] && { usage; exit 0; }\nUse project‑relative paths (not absolute)\nMake scripts portable by computing the project root from the script’s location.\nSCRIPT_DIR=$(cd -- \"$(dirname -- \"${BASH_SOURCE[0]}\")\" && pwd)\nPROJECT_ROOT=$(cd -- \"$SCRIPT_DIR/..\" && pwd)\nIN=\"$PROJECT_ROOT/data/input.txt\"\nOUTDIR=\"$PROJECT_ROOT/results\"\nNever overwrite outputs by accident\nCreate folders you need, and require --force to overwrite existing files.\nmkdir -p \"$OUTDIR\"\nOUT=\"$OUTDIR/summary.txt\"\nforce=false\n[[ ${1:-} == \"--force\" ]] && force=true\nif [[ -e \"$OUT\" && \"$force\" != true ]]; then\n  echo \"Refusing to overwrite $OUT (use --force)\"; exit 1\nfi\nLog what you do\nPrint key parameters and tool versions to a small log so runs are traceable.\n{\n  echo \"# Params\"; echo \"input=$IN\"; echo \"outdir=$OUTDIR\"; date\n  echo \"# Tool versions\"; bash --version | head -n1\n} | tee \"$OUTDIR/run.log\"\nQuote variables\nQuote \"$var\" to handle spaces.\n\n\n\n\n\n\n\nDid You Know?\n\n\n\n\nset -x prints each command as it runs — handy for debugging.\n\ncd - jumps back to your previous directory.\n\n\n\n\n\n\n\n\n\nBest Practice\n\n\n\nKeep scripts small and single‑purpose: one script = one task. Chain tasks with a Makefile later if needed."
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#common-mistakes-and-quick-fixes",
    "href": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#common-mistakes-and-quick-fixes",
    "title": "Best Coding Practices for Command‑Line Programming",
    "section": "3 Common Mistakes (and quick fixes)",
    "text": "3 Common Mistakes (and quick fixes)\n\nUnquoted variables → Always use \"$var\" (handles spaces safely).\n\nAbsolute paths → Use project‑relative paths like \"$PROJECT_ROOT/data\" instead.\n\nOverwriting results → Require --force or new output filenames.\n\nNo log → Write a minimal run.log with parameters and versions.\n\n\n\n\n\n\n\nCommon Mistake\n\n\n\nUsing rm -rf \"$SOME_VAR\" without checking that \"$SOME_VAR\" is non‑empty can delete the wrong folder. Always validate inputs first."
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#summary",
    "href": "practicals/Practicals - CLI/PRACTICAL_BEST_CODING_PRACTISES.html#summary",
    "title": "Best Coding Practices for Command‑Line Programming",
    "section": "4 Summary",
    "text": "4 Summary\n\nStart safely (set -euo pipefail, quote variables).\n\nUse project‑relative paths and don’t overwrite results.\n\nLog what you do; test on a small sample first.\n\nKeep scripts simple, readable, and single‑purpose."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIO511 Genomics - Practical Repository",
    "section": "",
    "text": "This repository contains practical exercises for the BIO511 Genomics course. Use this page, or the navbar to navigate to the different sections.\n\n\nHere are the practical exercises related to command line programming section of the course:\n\n\n\nPractical 1: CLI Basics\nPractical 2: Setting up the HPC\nPractical 3: Workspace Management and Data Management\nPractical 4: Scripting and Advanced Text Manipulation\nPractical 5: Software & Containers\n\n\n\n\n\nPractical 1: Git Fundamentals: Session 1\nPractical 2: Git Fundamentals: Session 2\nPractical 3: Best Coding Practices\n\n\n\n\n\nThe following practical exercises focus on genomics workflows:\n\nPractical 1: Quality Control and Classification\nPractical 2: Genome Assembly and Mapping\nPractical 3: Genome Annotation\n\n\n\n\nLastly, we cover Python programming with these practical exercises:\n\nPractical 0: Introduction\nPractical 1: Python Data Types and If/Else Clauses\nPractical 2: Python Loops\nPractical 3: Python Functions\nPractical 4: Python Modules\nPractical 5: Files\n\n\nFor questions or issues, please contact the course instructors."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "BIO511 Genomics - Practical Repository",
    "section": "",
    "text": "This repository contains practical exercises for the BIO511 Genomics course. Use this page, or the navbar to navigate to the different sections.\n\n\nHere are the practical exercises related to command line programming section of the course:\n\n\n\nPractical 1: CLI Basics\nPractical 2: Setting up the HPC\nPractical 3: Workspace Management and Data Management\nPractical 4: Scripting and Advanced Text Manipulation\nPractical 5: Software & Containers\n\n\n\n\n\nPractical 1: Git Fundamentals: Session 1\nPractical 2: Git Fundamentals: Session 2\nPractical 3: Best Coding Practices\n\n\n\n\n\nThe following practical exercises focus on genomics workflows:\n\nPractical 1: Quality Control and Classification\nPractical 2: Genome Assembly and Mapping\nPractical 3: Genome Annotation\n\n\n\n\nLastly, we cover Python programming with these practical exercises:\n\nPractical 0: Introduction\nPractical 1: Python Data Types and If/Else Clauses\nPractical 2: Python Loops\nPractical 3: Python Functions\nPractical 4: Python Modules\nPractical 5: Files\n\n\nFor questions or issues, please contact the course instructors."
  },
  {
    "objectID": "Pre-course-setup_BIO511.html",
    "href": "Pre-course-setup_BIO511.html",
    "title": "Pre-course setup",
    "section": "",
    "text": "Here is a tutorial to guide you through how to set up your computers for the BIO511 and following courses. Throughout the course we will both teach you how to use your local computer for bioinformatics but also, for more heavy duty stuff, use the c3se high performance computing cluster (HPC). The setup will therefore prepare you for both cases.\nSome of you might already have done some kinds of bioinformatics and have other setups and if you are comfortable with those, that’s fine by us. We will however not be able to give support/trouble shoot different kinds of setups so we ask you to make sure you have what’s below as a minimum."
  },
  {
    "objectID": "Pre-course-setup_BIO511.html#welcome-to-the-world-of-bioinformatics",
    "href": "Pre-course-setup_BIO511.html#welcome-to-the-world-of-bioinformatics",
    "title": "Pre-course setup",
    "section": "",
    "text": "Here is a tutorial to guide you through how to set up your computers for the BIO511 and following courses. Throughout the course we will both teach you how to use your local computer for bioinformatics but also, for more heavy duty stuff, use the c3se high performance computing cluster (HPC). The setup will therefore prepare you for both cases.\nSome of you might already have done some kinds of bioinformatics and have other setups and if you are comfortable with those, that’s fine by us. We will however not be able to give support/trouble shoot different kinds of setups so we ask you to make sure you have what’s below as a minimum."
  },
  {
    "objectID": "Pre-course-setup_BIO511.html#setup-for-mac-linux-users",
    "href": "Pre-course-setup_BIO511.html#setup-for-mac-linux-users",
    "title": "Pre-course setup",
    "section": "2 Setup for Mac / Linux users",
    "text": "2 Setup for Mac / Linux users\nmacOS is Unix-based (specifically, it’s a certified Unix system built on BSD) and therefore one can run many Linux-compatible tools without modification directly. You can basically just use the built in Terminal app and start coding. The Terminal runs the zsh shell by default.\nHowever, to get a better user experience, especially when we further on are going to access the HPC we will recommend using the integrated development environment (IDE) Visual Studio Code (VS Code). Please follow these instructions to do so."
  },
  {
    "objectID": "Pre-course-setup_BIO511.html#setup-for-windows-users",
    "href": "Pre-course-setup_BIO511.html#setup-for-windows-users",
    "title": "Pre-course setup",
    "section": "3 Setup for Windows users",
    "text": "3 Setup for Windows users\nUsing a Windows computer for bioinformatic work has sadly not been ideal most of the time, but large advances in recent years have made this quite feasible through the Windows Subsystem for Linux (WSL).\nApart from the Linux kernel provided by WSL, we also recommend installing the integrated development environment (IDE) Visual Studio Code (VS Code), and the tutorial will guide you through how to set up WSL integrated in VS Code. If you already feel comfortable with another IDE such and PyCharm since before, that’s ok but we may not be able to trouble shoot your setup during the course.\nThere are two substantially different versions of the Linux subsystem, WSL1 and WSL2. We strongly recommend using WSL2, which offers an essentially complete Linux experience and better performance.\nUsing the Linux subsystem will give you access to a full command-line bash shell and a Linux implementation on your Windows 10 or 11 PC. For the difference between the Linux Bash Shell and the Windows PowerShell, see e.g. this article.\nThe installation has three main steps:\n\nInstall WSL2 on Windows 10 or 11, follow the instructions here: Installing the Windows Subsystem and the Linux Bash\n\n\n\n\n\n\n\nNote\n\n\n\nIf you run into error messages when trying to download files through the Linux shell (e.g. curl:(6) Could not resolve host) then try adding the Google name server to the internet configuration by running sudo nano /etc/resolv.conf then add nameserver 8.8.8.8 to the bottom of the file and save it.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhenever a setup instruction specifies Mac or Linux (i.e. only those two, with no alternative for Windows), please follow the Linux instructions.\n\n\n\nInstall Visual Studio Code on the Windows side (not in WSL).\n\n\n\n\n\n\n\nNote\n\n\n\nNote: When prompted to Select Additional Tasks during installation, be sure to check the Add to PATH option so you can easily open a folder in WSL using the code command.\n\n\n\nInstall the WSL extension. If you plan to work with other remote extensions in VS Code, you may choose to install the Remote Development extension pack."
  },
  {
    "objectID": "Pre-course-setup_BIO511.html#getting-access-to-the-high-performance-computing-cluster-c3se",
    "href": "Pre-course-setup_BIO511.html#getting-access-to-the-high-performance-computing-cluster-c3se",
    "title": "Pre-course setup",
    "section": "4 Getting access to the High Performance computing cluster c3se",
    "text": "4 Getting access to the High Performance computing cluster c3se\nNow you have your local computer setup for bioinformatics. However, for many of the practicals we will use a high performance computing cluster, both so that you get familiar with that way of working, to keep a more controlled environment for you in the beginning and to let you perform analyses that require a bit more heavy duty computing power than your local machines.\nThis setup will be the same regardless of your OS.\n\nRegister yourself at SUPR. SUPR is the database for the National Swedish Infrastructure for Supercomputing (NAISS) and SUPR is used to keep track of persons, projects, project proposals and more. Please register with Federated Identity using your GU email address.\nWhen you have registered, please send an email to kaisa.thorell@gu.se and I will add you to the c3se project."
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html",
    "title": "Practical 3 — Workspace & Data Management",
    "section": "",
    "text": "The goal of this practical is to build good workspace habits on the command line so your projects are organized, reproducible, and HPC-friendly. You’ll practice:\n\nProject layouts that keep data, scripts, and results separate\n\nUsing symlinks to reference large/shared data (don’t duplicate!)\n\nFinding/searching files efficiently\n\nCombining commands with pipes and redirection\n\nManaging basic file permissions\n\nUnderstanding how these habits translate to the HPC filesystem\n\n\nImportant\nWe reuse the dataset you created in Practical 1 — the FASTA, gene annotations, and expression data files. Here, we will reorganize them into a structured project layout. If some of them still have different names (such as an E_coli prefix), please rename them back to the original filenames for consistency with this practical.\n\n\n\n\n\n\n\nDid You Know?\n\n\n\nMost bioinformatics tools are tiny LEGO bricks on the CLI. You connect bricks with pipes | to build powerful, reproducible analyses.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 3: Workspace Management and Data Management"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#locate-by-nametypesize-with-find",
    "href": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#locate-by-nametypesize-with-find",
    "title": "Practical 3 — Workspace & Data Management",
    "section": "4.1 Locate by name/type/size with find",
    "text": "4.1 Locate by name/type/size with find\ncd ~/CLI_intro\nfind . -name \"*.tsv\"           # all TSV files\nfind data -type f -size +1M    # files &gt; 1 MB in data/\nfind . -maxdepth 2 -type d     # directories up to depth 2",
    "crumbs": [
      "Intro to Command Line",
      "Practical 3: Workspace Management and Data Management"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#peek-safely-with-head-tail-less",
    "href": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#peek-safely-with-head-tail-less",
    "title": "Practical 3 — Workspace & Data Management",
    "section": "4.2 Peek safely with head, tail, less",
    "text": "4.2 Peek safely with head, tail, less\nWe assume data/metadata.tsv exists, but otherwise create it.\n# Show a small slice so it's visibly different from the full file:\n# If you need to create the metadata.tsv file: \ncat &gt; data/metadata.tsv &lt;&lt; 'EOF'\nSample_ID    Country Tissue\nS1   SE   gut\nS2   SE   gut\nS3   NO   skin\nS4   DK   gut\nS5   SE   skin\nEOF\n\nhead -n 2 data/metadata.tsv\ntail -n 2 data/metadata.tsv\nless data/metadata.tsv   # press 'q' to quit\n\n\n\n\n\n\nDefaults\n\n\n\nBy default, head and tail each print 10 lines if you don’t specify -n.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 3: Workspace Management and Data Management"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#search-inside-files-with-grep",
    "href": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#search-inside-files-with-grep",
    "title": "Practical 3 — Workspace & Data Management",
    "section": "4.3 Search inside files with grep",
    "text": "4.3 Search inside files with grep\n# Case-insensitive search for the word \"case\" in metadata\ngrep -i \"case\" data/metadata.tsv\n\n# Count the number of FASTA headers (if you have a FASTA in refs/)\ngrep -c \"^&gt;\" refs/sequences.fasta\n\n\n\n\n\n\nCommon Mistake\n\n\n\nIf the command says “No such file or directory”, ensure you’re in ~/CLI_intro and the file really exists there. Use pwd and ls to verify.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 3: Workspace Management and Data Management"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#counts-and-quick-summaries-with-wc-sort-uniq",
    "href": "practicals/Practicals - CLI/PRACTICAL_WORKSPACE_MANAGEMENT.html#counts-and-quick-summaries-with-wc-sort-uniq",
    "title": "Practical 3 — Workspace & Data Management",
    "section": "4.4 Counts and quick summaries with wc, sort, uniq",
    "text": "4.4 Counts and quick summaries with wc, sort, uniq\n\nCorrect mental model:\nsort orders lines (alphabetically/numerically). uniq only removes adjacent duplicates — so you almost always sort first before uniq.\n\n# How many rows (lines) are in the metadata?\nwc -l data/metadata.tsv\n\n# STEP-BY-STEP (encouraged so you see each stage):\ncut -f2 data/metadata.tsv              # column 2 (e.g., country) from a TAB-separated file\ncut -f2 data/metadata.tsv | sort       # order values to cluster identical ones\ncut -f2 data/metadata.tsv | sort | uniq -c   # collapse adjacent duplicates and count\nNow that you’ve seen each stage, try the pipeline form that saves the result:\ncut -f2 data/metadata.tsv | sort | uniq -c | sort -nr &gt; results/country_counts.txt\nhead results/country_counts.txt\n\n\n\n\n\n\nDelimiter tip\n\n\n\ncut defaults to tab. If your file is CSV, add -d, (comma) and adjust field numbers accordingly.",
    "crumbs": [
      "Intro to Command Line",
      "Practical 3: Workspace Management and Data Management"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html",
    "title": "Practical 1: Introduction to Command Line Interface",
    "section": "",
    "text": "Doing bioinformatics often necessitates the handling of very large files, with complex structure and dense matrices. This makes them often unwieldy to pretty much impossible to handle in traditional GUI-based analysis tools (such as Excel), and as such we rely on original and lightweight file handling systems such as bash-based command line enviroments that allow us to interact with our computer on a more fundamental level.\nTypically the first thing you encounter on opening a new terminal instance is:\nuser@computer:~$\nThis is the terminal prompt where you’ll be able to issue commands to the computer, with the syntax (“grammar” of the code) of your command prompts often using the following structure:\nuser@computer:-$ command -option my_object\n\n# or you may have more advanced command prompts\n\nuser@computer:-$ command -threads 2 -output $My_Directory_Variable/ -prefix New_file_ -accessoryfile Additional_file my_object | nextcommand -cvfz New_file_my_object.tar.gz\nImagination (and resources, time complexity, optimization…) sets the limit of what you can do in the CLI\nIf you’re further curious on the history of shell and the command line.\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this practical, you should be able to:\n\nNavigate the file system using cd and ls\nCreate and manipulate directories with mkdir\nCreate, copy, move, and delete files using touch, cp, mv, and rm\nView file contents using cat, head, and tail\nSearch within files using grep\nDisplay text using echo\nTie these commands together using pipes",
    "crumbs": [
      "Intro to Command Line",
      "Practical 1: CLI Basics"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#understanding-your-location-pwd-and-ls",
    "href": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#understanding-your-location-pwd-and-ls",
    "title": "Practical 1: Introduction to Command Line Interface",
    "section": "2.1 Understanding Your Location (pwd and ls)",
    "text": "2.1 Understanding Your Location (pwd and ls)\nBefore we start creating new things in our directory its a good idea to get a grip on your whereabouts within the working directory\n\npwd: Print working directory - shows your current location\nls: List contents of directories\n\nFind your current location and explore the directory\n# Run pwd\npwd\n\n# Run ls\nls \n\n# Run the ls help command, then find the option to let you\n# display the directory contents as a list with more details\nls --help\n\n# Then run that command\nls -la",
    "crumbs": [
      "Intro to Command Line",
      "Practical 1: CLI Basics"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#making-and-changing-directories-mkdir-and-cd",
    "href": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#making-and-changing-directories-mkdir-and-cd",
    "title": "Practical 1: Introduction to Command Line Interface",
    "section": "2.2 Making and Changing Directories (mkdir and cd)",
    "text": "2.2 Making and Changing Directories (mkdir and cd)\nSince the command line interface (CLI) can quickly become cluttered, we organize our work into directories. Let’s create a simple directory structure.\n\nmkdir: Create new directories\ncd: Change directory\nSpecial directories: . (current), .. (parent), ~ (home), / (root)\n\nCreate and navigate a genomics project structure\n# Create a main directory\nmkdir CLI_intro\n\n# Navigate into it\ncd CLI_intro\n\n# Create additional directories within the main directory\n# This can be done in multiple ways \n# For example, you could make multiple directories in one command:\nmkdir Escherichia\ncd Escherichia\nmkdir coli hermanni fergusonii\n\n# Check your structure\nls -la\n\n# Navigate around and try the special directory short-hands\ncd ~        # Go home\ncd ..        # Go back to previous directory",
    "crumbs": [
      "Intro to Command Line",
      "Practical 1: CLI Basics"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#text-manipulation-echo-touch",
    "href": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#text-manipulation-echo-touch",
    "title": "Practical 1: Introduction to Command Line Interface",
    "section": "3.1 Text manipulation (echo, touch)",
    "text": "3.1 Text manipulation (echo, touch)\nBefore we start with file manipulation, we’ll first introduce text files the old school way via echo and touch\n\ntouch: Create empty files\necho: Print text provided in the prompts\n\nHandling text in CLI\n# First, lets use echo in the ye olde intro to programming way\necho \"Hello World!\"\n\n# Not the most exciting, but we can use it to add text to files via &gt; \n# Create a file using touch \ntouch test_file\n\n# Then add some text to the file \necho \"Some_text\" &gt; test_file\n\n# Now add some more text to the next line in the file\n# To add to a file you simply extend the &gt; operator to &gt;&gt; \necho \"Some_more_text\" &gt;&gt; test_file\n\n# Now lastly, try to wipe the file with some new text \necho \"New_text\" &gt; test_file",
    "crumbs": [
      "Intro to Command Line",
      "Practical 1: CLI Basics"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#file-manipulation-cp-mv-rm",
    "href": "practicals/Practicals - CLI/PRACTICAL_INTRO_TO_COMMAND_LINE.html#file-manipulation-cp-mv-rm",
    "title": "Practical 1: Introduction to Command Line Interface",
    "section": "3.2 File Manipulation (cp, mv, rm)",
    "text": "3.2 File Manipulation (cp, mv, rm)\nNow let’s create some mock files that we will use downstream in the analysis, and learn how to use commands to copy, rename and remove files\n\nmv: Rename the absolute path of a file (moving it)\ncp: Copy a file\nrm: Remove a file (Irreversible, be careful!!)\n\nCreate sample files and practice file operations\n# Create some more directories to store everything in \nmkdir -p raw_data backups\n\n# Create some empty mock files for different data types that we'll use later \ntouch sequences.fasta\ntouch expression_data.csv\ntouch gene_annotations.csv\n\n# Add some mock sequence data \necho \"&gt;lacZ_beta-galactosidase\" &gt; sequences.fasta\necho \"ATGACCATGATTACGCCAAGCT\" &gt;&gt; sequences.fasta\necho \"&gt;rpoB_RNA_polymerase_beta\" &gt;&gt; sequences.fasta\necho \"ATGGTGACGACGACGACGATGCT\" &gt;&gt; sequences.fasta\necho \"&gt;gyrA_DNA_gyrase_subunit_A\" &gt;&gt; sequences.fasta\necho \"ATGGCTGCTGATCGATCGATGCTA\" &gt;&gt; sequences.fasta\necho \"&gt;ompA_outer_membrane_protein\" &gt;&gt; sequences.fasta\necho \"ATGCTGATCGATCGGCTAGCTAGC\" &gt;&gt; sequences.fasta\n\n# Add data to the gene annotation file \necho \"Gene_ID,Chromosome,Start,End,Strand,Product\" &gt; gene_annotations.csv\necho \"lacZ,plasmid,1000,2500,+,beta-galactosidase\" &gt;&gt; gene_annotations.csv\necho \"rpoB,chr,3000,4200,+,RNA polymerase beta subunit\" &gt;&gt; gene_annotations.csv\necho \"gyrA,chr,500,1800,-,DNA gyrase subunit A\" &gt;&gt; gene_annotations.csv\necho \"ompA,chr,2000,3500,+,outer membrane protein A\" &gt;&gt; gene_annotations.csv\necho \"LacY,plasmid,5000,6200,+,Lactose_permease\" &gt;&gt; gene_annotations.csv\necho \"tolC,chr,100,800,+,outer membrane channel protein\" &gt;&gt; gene_annotations.csv\n\n# Add some gene expression data\necho \"Gene_ID,Condition_A,Condition_B,Function\" &gt; expression_data.csv\necho \"lacZ,150,200,beta-galactosidase\" &gt;&gt; expression_data.csv\necho \"rpoB,89,95,RNA_polymerase\" &gt;&gt; expression_data.csv\necho \"gyrA,175,180,DNA_gyrase\" &gt;&gt; expression_data.csv\necho \"LacY,165,210,Lactose_permease\" &gt;&gt; expression_data.csv\necho \"ompA,45,52,outer_membrane_protein\" &gt;&gt; expression_data.csv\n\n# Copy files (backup your data!)\ncp sequences.fasta sequences_backup.fasta\n\n# Move files to organize your project\nmv sequences_backup.fasta backups/\n\n# mv can also be used to rename files, similar to the syntax of cp \n# You just remembered that these files were E.coli, rename them and think of good informative names\nmv raw_data/gene_annotation.csv raw_data/E_coli_gene_annotations.csv\n\n# remove files (be careful, no undo!)\nrm sequences_backup.fasta\n# rm can also be used with flags to remove directories and their contents\nrm -r backups/\n\n# Lastly lets sort a bit in your directories\n# Copy the entire raw_data directory with contents (find the right flag!)\ncp -r raw_data/ Escherichia/coli/\nNote: After this section is done, remember that you renamed the data files to include the species name (E.coli), rename them back for the subsequent sections if you want. Otherwise, make sure the change doesnt confuse you",
    "crumbs": [
      "Intro to Command Line",
      "Practical 1: CLI Basics"
    ]
  },
  {
    "objectID": "practicals/Practicals - CLI/PRACTICAL_SETTING_UP_THE_HPC.html",
    "href": "practicals/Practicals - CLI/PRACTICAL_SETTING_UP_THE_HPC.html",
    "title": "Practical 2: Connecting to the HPC",
    "section": "",
    "text": "1 Introduction\nThis tutorial is to take you through the final steps of setting up and accessing the c3se High Performance Computing infrastructure. The specific cluster we will use is called Vera. You will be introduced to this cluster in the lecture and here you can find more information about it, including what resources that are available, how to run jobs and other documentation.\nWe will walk you through step by step in this tutorial so that we (hopefully) can assist if there are any problems.\n\n\n\n\n\n\nPre-course setup\n\n\n\nTo be able to follow these steps you first have to have followed the steps of the Pre-course Setup including registration at SUPR and application to the course HPC project.\n\n\n\n\n\n2 Adding Vera/c3se at SUPR\n\nLog in to SUPR\nClick “Accounts” in the left side menu\nRequest access to Vera/c3se\n\n\n\n\n3 Setting up a Chalmers ID\n\nNavigate to https://myaccount.chalmers.se/.\nLog in with your GU account.\nChoose “Activate CID” and follow the instructions to activate your account\n\n\n\n\n\n\n\nRemember your password!\n\n\n\nThe CID username and password are the ones you will use to log in to the cluster\n\n\n\n\n\n4 Setting up Chalmers VPN\nTo be able to access c3se from other networks than in GU or Chalmers, you will need a Chalmers VPN\n\nNavigate to https://chalmers.topdesk.net/tas/public/ssp/content/detail/knowledgeitem?unid=07e6154089a648d09711ccc42a42fc88\nFollow the instructions to set up the Chalmers VPN\n\n\n\n\n5 Connecting to the HPC using ssh in VSCode\nTo be able to connect to the HPC in a manner that will let you use the full suite of tools available (or most of them at least) in VSCode, you will need to install the Remote - SSH extension.\n\nOpen VSCode\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or by pressing Ctrl+Shift+X\nSearch for “Remote - SSH” and install the extension by Microsoft (if youre unsure how install extensions, see here)\nAfter installation, you will see a new icon in the Activity Bar on the side of the window. Click on it to open the Remote Explorer view.\n\nIf the SSH Targets are not visible, click on the drop down to expand the view\n\nClick on the + (“New Remote”) icon next to SSH header to add a new SSH host\nIn the input box that appears, enter the SSH connection string for Vera: &lt;your_chalmers_id&gt;@vera1.c3se.chalmers.se (replace &lt;your_chalmers_id&gt; with your actual Chalmers ID)\nPress Enter to confirm\nYou will be prompted to select the SSH configuration file to update. Choose the default option (usually ~/.ssh/config)\n\nOnce you have added the SSH host, you can connect to Vera:\n\nIn the Remote Explorer view, find the Vera entry under SSH Targets (or by clicking on the blue “remote window” icon in the bottom left corner of VSCode)\n\nClick on the “Connect to Host” icon (an arrow if you want the same window, square if you want a new one) next to the Vera entry\nIf you clicked on the blue “remote window” icon, select “Remote-SSH: Connect to Host…” and then select the Vera entry\n\nA new VSCode window will open, and you will be prompted to enter your Chalmers ID password\nAfter entering your password, you should be connected to Vera, and you can start working on the HPC cluster directly from VSCode. Make sure that you open your home folder on Vera to open up a workspace.\nYou can verify that you are connected to Vera by opening a new terminal in VSCode (Terminal &gt; New Terminal) and checking the hostname in the terminal prompt. It should show something like your_chalmers_id@vera:~$",
    "crumbs": [
      "Intro to Command Line",
      "Practical 2: Setting up the HPC"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html",
    "title": "Git Fundamentals: Session 1",
    "section": "",
    "text": "In this first session, you will learn the Git workflow: initialize a repository, make clean commits, inspect history, ignore generated files, and do a simple branch-and-merge.\nWhy this matters for bioinformatics - Reproducibility: record exactly what changed and when. - Safety: experiment on branches without breaking your main work. - Collaboration: a clear, searchable history others can follow.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\nBy the end of this session, you can: 1) Initialize a repo and configure your identity 2) Stage and commit with clear, focused messages 3) Inspect unstaged vs staged changes and view history 4) Use .gitignore to keep clutter out of Git 5) Create a short-lived feature branch, merge it, and clean up",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#introduction",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#introduction",
    "title": "Git Fundamentals: Session 1",
    "section": "",
    "text": "In this first session, you will learn the Git workflow: initialize a repository, make clean commits, inspect history, ignore generated files, and do a simple branch-and-merge.\nWhy this matters for bioinformatics - Reproducibility: record exactly what changed and when. - Safety: experiment on branches without breaking your main work. - Collaboration: a clear, searchable history others can follow.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\nBy the end of this session, you can: 1) Initialize a repo and configure your identity 2) Stage and commit with clear, focused messages 3) Inspect unstaged vs staged changes and view history 4) Use .gitignore to keep clutter out of Git 5) Create a short-lived feature branch, merge it, and clean up",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#main-concepts",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#main-concepts",
    "title": "Git Fundamentals: Session 1",
    "section": "2 Main Concepts",
    "text": "2 Main Concepts\n\nRepository (repo): a project folder that Git watches (contains a hidden .git/ directory).\nWorking tree: your actual files on disk.\nStaging area (index): a waiting room where you queue the exact changes for your next snapshot.\nCommit: a permanent snapshot with a message (what/why).\nBranch: a label that moves forward as you commit, representing a line of work (e.g., main, feature/x).\n.gitignore: patterns telling Git which files/folders not to track (e.g., results/).\n\n\n\n\n\n\n\nDid You Know?\n\n\n\nYou can set the default branch name to main globally:\ngit config --global init.defaultBranch main",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#before-you-begin",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#before-you-begin",
    "title": "Git Fundamentals: Session 1",
    "section": "3 Before You Begin",
    "text": "3 Before You Begin\nConfirm Git & shell\ngit --version\nwhich git\nYou should see a version like git version 2.x and a path.\nSet your identity (appears in commit history)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n# Beginner-friendly editor for commit messages\ngit config --global core.editor \"nano -w\"\n# Verify\ngit config --list --show-origin\nTerminal refresher\npwd        # print working directory\nls -la     # list files (detailed)\ncd &lt;dir&gt;   # change directory\nmkdir &lt;d&gt;  # make directory\nnano &lt;f&gt;   # edit file (CTRL+O save, Enter, CTRL+X exit)\n\n\n\n\n\n\nCommon Mistake\n\n\n\nWorking in a path with spaces can cause quoting issues (e.g., My Documents). Prefer ~/projects/git-lab.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#step-by-step-practical",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#step-by-step-practical",
    "title": "Git Fundamentals: Session 1",
    "section": "4 Step-by-Step Practical",
    "text": "4 Step-by-Step Practical\n\nWe’ll create a small, clean project called git-lab and practice the local Git cycle repeatedly.\n\n\n4.1 Create a Repository\nmkdir -p ~/projects/git-lab && cd ~/projects/git-lab\ngit init\nExpected output: a line like Initialized empty Git repository in .../.git/\ngit status\nExpected: “No commits yet” and nothing staged.\n\n\n\n\n\n\nBest Practice\n\n\n\nRun git status often. It tells you exactly what Git sees at every step.\n\n\n\n\n4.2 Add Initial Files\nCreate a README, a small config, and the folder layout:\ncat &gt; README.md &lt;&lt; 'EOF'\n# Git Lab\nThis mini project demonstrates local Git basics.\n\n## Quickstart\nSee below for commands and checkpoints.\nEOF\n\nmkdir -p config scripts results data\n\ncat &gt; config/config.yml &lt;&lt; 'EOF'\nproject: git-lab\nthreads: 2\ngenome_id: ST398\nEOF\nSelf-check\nls -la\n# Expect: README.md, config/, scripts/, results/, data/\ncat config/config.yml\n\n\n4.3 Stage and First Commit\ngit add README.md config/config.yml\ngit status          # these files should appear under \"Changes to be committed\"\n\ngit commit -m \"Add README and base config\"\nVerify history\ngit log --oneline -n 1\n# Expect: one commit with your message\n\n\n\n\n\n\nCommon Mistake\n\n\n\nSeeing “nothing to commit”? You probably forgot git add. Stage your changes first, then commit.\n\n\n\n\n4.4 .gitignore to Keep the Repo Clean\ncat &gt; .gitignore &lt;&lt; 'EOF'\nresults/\n*.tmp\n.DS_Store\n*.swp\nEOF\n\ngit add .gitignore\ngit commit -m \"Add .gitignore for results and temp files\"\nProve it works\nmkdir -p results && echo \"tmp\" &gt; results/test.tmp\ngit status   # results/ and *.tmp should NOT show up\n\n\n\n\n\n\nDid You Know?\n\n\n\nIf a file was tracked before you added it to .gitignore, Git keeps tracking it. Use git rm --cached &lt;file&gt; once to stop tracking.\n\n\n\n\n4.5 Inspecting Changes — Unstaged vs Staged\nMake and review edits:\n# Edit values (macOS and Linux differ; we try both variants)\nsed -i '' 's/threads: 2/threads: 4/' config/config.yml 2&gt;/dev/null || sed -i 's/threads: 2/threads: 4/' config/config.yml\n\ngit status       # shows modified but unstaged\ngit diff         # shows the exact diff\n\ngit add config/config.yml\ngit diff --staged  # shows what will be committed\n\ngit commit -m \"Increase threads to 4\"\nGood commit messages (rule of thumb) - Subject ≤ 50 chars, imperative: “Increase threads to 4” - Body (optional): wrap at 72 chars; explain why\n\n\n\n\n\n\nBest Practice\n\n\n\nUse git add -p to stage only the meaningful hunks when a file has multiple changes.\n\n\n\n\n4.6 Branching Basics & No-Conflict Merge\nCreate a branch, make a small change, merge to main, then delete the branch:\ngit switch -c feature/readme-usage\n\necho \"\n## Usage\n./scripts/qc.sh ST398 4 &gt; results/qc.log\" &gt;&gt; README.md\n\ngit add README.md\ngit commit -m \"Document basic usage with example command\"\n\ngit switch main\ngit merge feature/readme-usage\n\ngit branch -d feature/readme-usage\nVerify\ngit log --oneline --decorate --graph --all | head -n 10\nYou should see a merge into main and the feature branch deleted locally.\n\n\n\n\n\n\nCommon Mistake\n\n\n\nMerging fails if you have uncommitted work. Commit or stash changes first; then merge.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#guided-practice-you-do-it",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#guided-practice-you-do-it",
    "title": "Git Fundamentals: Session 1",
    "section": "5 Guided Practice (You Do It)",
    "text": "5 Guided Practice (You Do It)\nComplete these tasks in your git-lab repo. Use the self-checks and commands shown earlier.\n\nParameter tweak\nEdit config/config.yml to add memory_gb: 8. Stage and commit with a clear message.\nSecond feature branch\nCreate feature/genome-note, append notes: update genome soon to config/config.yml, commit, merge into main, and delete the branch.\nProve .gitignore works\nCreate results/run1.log and scratch.tmp. Show that git status ignores them.\nInspect history\nRun git log --oneline --decorate --graph -n 5 and briefly explain (in your own words) what the last 3 commits did.\n\n\n\n\n\n\n\nBest Practice\n\n\n\nKeep branches short-lived and focused on one small change. Merge when done; delete the branch to avoid clutter.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#self-check-quiz-quick",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#self-check-quiz-quick",
    "title": "Git Fundamentals: Session 1",
    "section": "6 Self-Check Quiz (Quick)",
    "text": "6 Self-Check Quiz (Quick)\n\nWhat’s the difference between unstaged and staged changes?\n\nWhat does .gitignore do? What doesn’t it do?\n\nWhy write commit messages in imperative mood (“Add”, “Fix”)?\n\nWhat command shows staged changes only?\n\nHow do you create and switch to a new branch in one command?\n\n(Answers: 1) Unstaged are edits on disk; staged are queued for the next commit. 2) Prevents new files matching patterns from being tracked; doesn’t untrack files already committed. 3) Reads as commands to the codebase—clear history. 4) git diff --staged. 5) git switch -c &lt;name&gt;.)",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#troubleshooting-recovery",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#troubleshooting-recovery",
    "title": "Git Fundamentals: Session 1",
    "section": "7 Troubleshooting & Recovery",
    "text": "7 Troubleshooting & Recovery\n\n“fatal: not a git repository” → Run pwd and ensure you’re in ~/projects/git-lab. Use cd to enter the repo folder.\nsed -i complains → Open the file with nano config/config.yml, edit manually, save.\nAccidentally staged the wrong file → git restore --staged &lt;file&gt; (does not change file content).\nWant to start over → Move away from the folder (or delete it) and repeat from step 1. To un-Git a folder, remove the hidden .git/ directory.\n\n\n\n\n\n\n\nDid You Know?\n\n\n\ngit show &lt;commit&gt; reveals the patch and metadata for a single commit. Great for code review and learning from changes.",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#summary",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#summary",
    "title": "Git Fundamentals: Session 1",
    "section": "8 Summary",
    "text": "8 Summary\nYou created a clean repository, practiced the local cycle (status → diff → add → commit → log), used .gitignore to keep noise out, and completed a simple branch-and-merge. These habits set the foundation for Session 2 (conflicts, tags, remotes, and recovery).",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#cheat-sheet",
    "href": "practicals/Practicals - Git/PRACTICAL_GIT_SESSION1.html#cheat-sheet",
    "title": "Git Fundamentals: Session 1",
    "section": "9 Cheat Sheet",
    "text": "9 Cheat Sheet\nInitialize:  git init\nIdentity:    git config --global user.name \"Name\"; git config --global user.email \"you@ex\"\nStatus:      git status\nStage:       git add &lt;file&gt;\nCommit:      git commit -m \"Message\"\nDiff:        git diff        (unstaged)   |   git diff --staged\nLog:         git log --oneline --decorate --graph\nIgnore:      echo pattern &gt;&gt; .gitignore; git add .gitignore; git commit -m \"Add .gitignore\"\nBranch:      git switch -c &lt;name&gt;; git switch main; git merge &lt;name&gt;; git branch -d &lt;name&gt;",
    "crumbs": [
      "Coding Best Practices",
      "Practical 1: Git Fundamentals: Session 1"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "",
    "text": "Annotation is the process of taking assembled genome sequences (or in some cases unassembled sequences) and identifying genomic features such as genes, coding sequences (CDS), tRNAs, rRNAs, and other functional elements. This is a critical step in guiding deeper analysis, as the annotated genes become guidemarks for functional identification and characterizing phenotypes. It is also typically the first step before more advanced downstream analysis after we’ve finished assembly and polishing. With annotated genomes, we can perform comparative genomics, functional profiling, and pangenome analyses.\n\n\n\n\n\n\nREMEMBER: Create a separate project folder\n\n\n\n# Remember to make separate project folders for this practical\n# How you structure your workspace is up to you, but make sure to adjust the paths in the scripts below accordingly\n# EXAMPLE:\nmkdir -p ~yourdir/bio511/practical_2\n\n# Symlink the data folder to your project directory\nln -s /cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data ./data\n\n# Or if you need to use the data locally, copy it\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data ./data\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end you should be able to:\n\nRun Prokka to annotate prokaryotic genomes\nProvide curated reference fasta-files to improve annotation quality\nUse MultiQC to aggregate and compare annotation outputs\nPerform basic inspection of annotation results using command-line tools and MultiQC",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#basicagnostic-prokka-annotation",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#basicagnostic-prokka-annotation",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "2.1 Basic/agnostic Prokka annotation",
    "text": "2.1 Basic/agnostic Prokka annotation\nFor the first run, we’ll perform a basic annotation using Prokka’s default settings. This will give us a general overview of the genome’s features using the built-in databases.\n#!/bin/bash\n#SBATCH -A C3SE408-25-2 \n#SBATCH -J prokka_baseline\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=8\n#SBATCH -t 01:00:00\n#SBATCH --output=/cephyr/users/youruser/Vera/practical_3/logs/prokka_baseline_%j.out # output log, adjust path to where you want to save logs\n#SBATCH --error=/cephyr/users/youruser/Vera/practical_3/logs/prokka_baseline_%j.err # error log, adjust path to where you want to save logs\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nCONTAINER_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/prokka.sif\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_3\"\nASSEMBLIES_DIR=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/assemblies\"\nOUTPUT_DIR=\"${RESULTS_PATH}/annotation/prokka_baseline\"\n\n# Bind paths for container\nexport SINGULARITY_BINDPATH=\"${ASSEMBLIES_DIR}:/assemblies,${RESULTS_PATH}:/results\"\n\n# Create output directories\n# If needed: mkdir -p ${RESULTS_PATH}\nmkdir -p ${OUTPUT_DIR}\n\n# Input assembly\nASSEMBLY=\"/assemblies/GU10.fa\"\n\n# Set a prefix variable for naming outputs\nPREFIX=\"$(basename ${ASSEMBLY} .fa)_baseline\"\n\n# Run Prokka (baseline)\nsingularity exec ${CONTAINER_PATH} prokka \\\n  --cpus 8 \\\n  --outdir /results/annotation/prokka_baseline \\\n  --prefix ${PREFIX} \\\n  --locustag BIO511 \\\n  --genus Escherichia \\\n  --species coli \\\n  ${ASSEMBLY}\n\necho \"Baseline Prokka annotation completed\"",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#prokka-with-curated-proteins",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#prokka-with-curated-proteins",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "2.2 Prokka with curated proteins",
    "text": "2.2 Prokka with curated proteins\nFor the second run, we’ll enhance the annotation by providing Prokka with a curated set of reference proteins. This can help improve the accuracy of gene predictions and functional annotations, The curated proteins used for this ref are a collection of reference genomes for E. coli from NCBI RefSeq, which have been manually reviewed and merged. Using these curated proteins can help reduce the number of hypothetical proteins and improve the functional annotations of genes in our genome.\nStrains:\n\nEscherichia coli K-12 MG1655 (GCF_000005845.2)\nEscherichia coli O157:H7 str. Sakai (GCF_000008865.2)\nEscherichia coli ATCC 25922 (GCF_000401755.1)\nEscherichia coli O25b:H4-ST131 (GCF_024918415.1)\n\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J prokka_curated\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=8\n#SBATCH -t 01:00:00\n#SBATCH --output=/cephyr/users/youruser/Vera/practical_3/logs/prokka_curated_%j.out # output log, adjust path to where you want to save logs\n#SBATCH --error=/cephyr/users/youruser/Vera/practical_3/logs/prokka_curated_%j.err # error log, adjust path to where you want to save logs\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nCONTAINER_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/prokka.sif\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_3\"\nASSEMBLIES_DIR=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/assemblies\"\nCURATED_PROTEINS=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/curated_refs/curatedRef.faa\"\nOUTPUT_DIR=\"${RESULTS_PATH}/annotation/prokka_curated\"\n\n# Bind paths for container\nexport SINGULARITY_BINDPATH=\"${ASSEMBLIES_DIR}:/assemblies,${RESULTS_PATH}:/results,${CURATED_PROTEINS}:/curatedRef.faa\"\n\n# Create output directories\nmkdir -p ${OUTPUT_DIR}\n\n# Input assembly\nASSEMBLY=\"/assemblies/GU10.fa\"\n\n# Set a prefix variable for naming outputs\nPREFIX=\"$(basename ${ASSEMBLY} .fa)_curated\"\n\n# Run Prokka (curated)\nsingularity exec ${CONTAINER_PATH} prokka \\\n  --cpus 8 \\\n  --outdir /results/annotation/prokka_curated \\\n  --prefix ${PREFIX} \\\n  --locustag BIO511 \\\n  --genus Escherichia \\\n  --species coli \\\n  --proteins /curatedRef.faa \\\n  ${ASSEMBLY}\n\necho \"Curated Prokka annotation completed\"\nSubmit jobs and monitor:\nsbatch prokka_baseline_job.slurm\nsbatch prokka_curated_job.slurm\nsqueue -u $USER",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#prepare-inputs-for-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#prepare-inputs-for-multiqc",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "3.1 Prepare inputs for MultiQC",
    "text": "3.1 Prepare inputs for MultiQC\n# Create a directory to collect Prokka outputs\nmkdir -p multiqc_analysis/prokka\n\n# Copy Prokka result folders\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_3/annotation/prokka_baseline ./multiqc_analysis/prokka/\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_3/annotation/prokka_curated ./multiqc_analysis/prokka/",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#run-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#run-multiqc",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "3.2 Run MultiQC",
    "text": "3.2 Run MultiQC\ncd multiqc_analysis\n\nmultiqc . \\\n  --title \"BIO511 Practical 3 - Prokka Annotation\" \\\n  --filename \"practical3_prokka_report\" \\\n  --dirs \\\n  --dirs-depth 2 \\\n  --force",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#review-questions",
    "href": "practicals/Practicals - WGS/PRACTICAL_FUNCTIONAL_ANNOTATION.html#review-questions",
    "title": "Practical 3: Functional Annotation of Whole Genomes",
    "section": "3.3 Review Questions",
    "text": "3.3 Review Questions\nOpen the HTML report and review the Prokka section comparing baseline vs curated. Use this report, manual data observation and any other relevant methods to answer the questions below:\n\nHow many CDS, tRNA, and rRNA features were called in each run? Why dont we see a difference?\nHow many hypothetical proteins in baseline vs curated? What changed? (hint: Remember a certain script you made to look up a certain pattern in ffn files? or could you use grep or awk?)\nWhich genes had improved product names with curated proteins? Name some examples. (hint: Open and look at the .tsv files)\nWhat additional curated sets could you add for better annotations?",
    "crumbs": [
      "Practical 3: Genome Annotation"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "",
    "text": "After quality control, the next critical step in genomic analysis is either mapping reads to a reference genome or performing de novo assembly to reconstruct genomes from scratch. This practical will both processes in a sense, but focus will be on de novo assembly using long reads from Oxford Nanopore Technologies (ONT) and assessing the quality of these assemblies. We will use tools like Flye for assembly, Quast for quality assessment, and MultiQC for comprehensive reporting. Additionally, we will also explore hybrid assembly to combine long and short reads for improved accuracy. For this we will use the Burrows-Wheeler Aligner (bwa), serving as a somewhat introduction to mapping reads to an assembled sequence.\nThe documentation for each tool used in this practical can be found here:\n\nFlye\nQuast\nMultiQC\nBWA\nPolypolish\n\n\n\n\n\n\n\nREMEMBER: Create a separate project folder\n\n\n\n# Remember to make separate project folders for this practical\n# How you structure your workspace is up to you, but make sure to adjust the paths in the scripts below accordingly\n# EXAMPLE:\nmkdir -p ~yourdir/bio511/practical_2\n\n# Symlink the data folder to your project directory\nln -s /cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data ./data\n\n# Or if you need to use the data locally, copy it\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data ./data\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this practical, you will be able to:\n\nUnderstand the differences between reference mapping and de novo assembly\nPerform de novo assembly using Flye for long reads\nAssess assembly quality using Quast\nGenerate comprehensive reports with MultiQC\nCombine long and short reads for hybrid assembly and polishing (optional)",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#a-summary",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#a-summary",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "2.1 A summary",
    "text": "2.1 A summary\nDe Novo Assembly is used commonly when:\n\nNo reference genome exists for your organism, or the reference is hard to get good coverage (ie. bacteria with high pangenomic diversity)\nStudying structural variations or novel sequences\nWorking with metagenomes or mixed communities (where maybe the a total genome is not really needed, rather the genetic content)\n\nReference Mapping/Assembly is preferred when:\n\nA high-quality reference genome is available\nThe goal is to identify variants relative to a known sequence\nWorking with well-characterized organisms\nOften less relevant to bacteria, due to the aforementioned pangenomic diversity",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#job-1-basic-ont-assembly",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#job-1-basic-ont-assembly",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "3.1 Job 1: Basic ONT Assembly",
    "text": "3.1 Job 1: Basic ONT Assembly\nFirst, let’s create a job script for a basic assembly with no polishing:\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J flye_basic\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=16\n#SBATCH -t 01:30:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_2/logs/flye_basic_%j.out\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_2/logs/flye_basic_%j.err\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nDATA_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq/Assembly_reads\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_2\"\nSINGULARITY_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/flye.sif\"\n\n# Set bind paths for Singularity\nexport SINGULARITY_BINDPATH=\"${DATA_PATH}:/data,${RESULTS_PATH}:/results\"\n\n# Set sample and output paths\nONT_READS=\"${DATA_PATH}/CAN-290_ONT.fastq.gz\"\nOUTPUT_DIR=\"${RESULTS_PATH}/assembly/flye_basic\"\n\n# Create output directories\nmkdir -p ${OUTPUT_DIR}\n\n# Run Flye assembly with basic settings\nsingularity exec ${SINGULARITY_PATH} flye --nano-raw /data/CAN-290_ONT.fastq.gz \\\n     --out-dir /results/assembly/flye_basic \\\n     --threads 16 \\\n     --iterations 0\n\necho \"Basic ONT assembly completed\"",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#job-2-polished-ont-assembly",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#job-2-polished-ont-assembly",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "3.2 Job 2: Polished ONT Assembly",
    "text": "3.2 Job 2: Polished ONT Assembly\nNow create a second job with more aggressive polishing parameters:\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J flye_polished\n#SBATCH -p vera  \n#SBATCH -N 1 --cpus-per-task=12\n#SBATCH -t 02:30:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_2/logs/flye_polished_%j.out\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_2/logs/flye_polished_%j.err\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS  \nDATA_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq/Assembly_reads\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_2\"\nSINGULARITY_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/flye.sif\"\n\n# Set bind paths for Singularity\nexport SINGULARITY_BINDPATH=\"${DATA_PATH}:/data,${RESULTS_PATH}:/results\"\n\n# Set sample and output paths\nONT_READS=\"${DATA_PATH}/CAN-290_ONT.fastq.gz\"\nOUTPUT_DIR=\"${RESULTS_PATH}/assembly/flye_polished\"\n\n# Create output directories\nmkdir -p ${OUTPUT_DIR}\n\n# Run Flye assembly with enhanced polishing\nsingularity exec ${SINGULARITY_PATH} flye --nano-raw /data/CAN-290_ONT.fastq.gz \\\n     --out-dir /results/assembly/flye_polished \\\n     --threads 12 \\\n     --iterations 5 \\\n\necho \"Polished ONT assembly completed\"\nSubmit both jobs:\n# Submit the assembly jobs\nsbatch flye_basic_job.sh\nsbatch flye_polished_job.sh\n\n# Monitor job status\nsqueue -u $USER",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#running-quast-for-assembly-statistics",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#running-quast-for-assembly-statistics",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "4.1 Running QUAST for Assembly Statistics",
    "text": "4.1 Running QUAST for Assembly Statistics\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J quast_analysis\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=4\n#SBATCH -t 01:00:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_2/logs/quast_%j.out\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_2/logs/quast_%j.err\n\n# Load or use containerized QUAST\nCONTAINER_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/quast.sif\"\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nDATA_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq/Assembly_reads\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_2\"\nOUTPUT_DIR=\"${RESULTS_PATH}/results/quast_results\"\n\n# Set the path to the assemblies\nASSEMBLIES_DIR=\"${RESULTS_PATH}/assembly\"\n\n# Bind paths for container\nexport SINGULARITY_BINDPATH=\"${ASSEMBLIES_DIR}:/assemblies,${OUTPUT_DIR}:/output\"\n\n# Create output directory\nmkdir -p ${OUTPUT_DIR}\n\n# Run QUAST on both assemblies\nsingularity exec ${CONTAINER_PATH} quast.py \\\n    -o ${OUTPUT_DIR} \\\n    --threads 4 \\\n    --plots-format png \\\n    --labels \"Basic,Polished\" \\\n    /assemblies/flye_basic/assembly.fasta \\\n    /assemblies/flye_polished/assembly.fasta\n\necho \"QUAST analysis completed\"",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#look-at-base-assembly-statistics",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#look-at-base-assembly-statistics",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "4.2 Look at base assembly statistics",
    "text": "4.2 Look at base assembly statistics\nWhile waiting for the QUAST job to finish, you can look at some basic assembly statistics provided by Flye in the assembly_info.txt file located in each assembly output directory, if you want. Make sure that you understand what each metric means. Google any terms you are unfamiliar with!",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#preparing-data-for-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#preparing-data-for-multiqc",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "5.1 Preparing Data for MultiQC",
    "text": "5.1 Preparing Data for MultiQC\n# Create a comprehensive results directory structure\nmkdir -p multiqc_analysis/{assembly_qc,quast_results,kraken2_results}\n\n# Copy QUAST results\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/students/name/results/quast_results/* multiqc_analysis/quast_results/\n\n# We have also prepared some kraken2 results for the reads that you can include in your report\nscp -r CID@vera2.c3se.chalmers.se:/cephyr/NOBACKUP/groups/n2bin_gu/teachers/Tor/extra_data/kraken2_results/* multiqc_analysis/kraken2_results/",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#running-multiqc",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#running-multiqc",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "5.2 Running MultiQC",
    "text": "5.2 Running MultiQC\n# Navigate to analysis directory\ncd multiqc_analysis\n\n# Run MultiQC to aggregate all results\nmultiqc . \\\n    --title \"BIO511 Practical 2 - Assembly Analysis\" \\\n    --filename \"practical2_assembly_report\" \\\n    --dirs \\\n    --dirs-depth 2 \\\n    --force",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#assembly-quality-metrics",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#assembly-quality-metrics",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "6.1 Assembly Quality Metrics",
    "text": "6.1 Assembly Quality Metrics\nUse your QUAST reports and MultiQC output to answer:\n\nWhat is the total assembly size for each assembly? How many contigs were generated in each assembly?\nWhat is the N50 value for each? What is L50, or L90? What do these metrics tell you about the assembly quality?\nHow do the basic and polished assemblies compare? Which assembly has better contiguity (fewer, longer contigs)?\nHow does the assembly size compare to the expected genome size? What is the expected genome size for H. pylori?\nWhat additional data or methods would you recommend to improve this assembly?",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#understanding-hybrid-assembly",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#understanding-hybrid-assembly",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "7.1 Understanding Hybrid Assembly",
    "text": "7.1 Understanding Hybrid Assembly\nAn example of a hybrid assembly workflow is:\n\nInitial assembly with long reads (Flye)\nMapping short reads to the assembly (BWA)\nFiltering alignments for optimal polishing\nPolishing the assembly using the filtered short reads (Polypolish)",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#setting-up-hybrid-assembly",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#setting-up-hybrid-assembly",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "7.2 Setting up Hybrid Assembly",
    "text": "7.2 Setting up Hybrid Assembly\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J hybrid_assembly\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=12\n#SBATCH -t 03:00:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_2/logs/hybrid_%j.out\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_2/logs/hybrid_%j.err\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nDATA_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/data/fastq/Assembly_reads\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_2\"\nFLYE_CONTAINER=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/flye.sif\"\nPP_CONTAINER=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/polypolish.sif\"\n\n# Set bind paths for Singularity\nexport SINGULARITY_BINDPATH=\"${DATA_PATH}:/data,${RESULTS_PATH}:/results\"\n\n# Set sample and output paths\nONT_READS=\"${DATA_PATH}/CAN-290_ONT.fastq.gz\"\nSHORT_R1=\"${DATA_PATH}/CAN-290_R1.fastp.fq.gz\"\nSHORT_R2=\"${DATA_PATH}/CAN-290_R2.fastp.fq.gz\"\nOUTPUT_DIR=\"${RESULTS_PATH}/hybrid_assembly/results\"\nSAMPLE_NAME=\"hybrid_assembly\"\n\n# Create output directory\nmkdir -p ${OUTPUT_DIR}\n\n# Run the first Flye assembly (if not already done)\n# If you have already done the Flye assembly in previous steps, you can skip this step by commenting it out\necho \"Step 1: Running initial Flye assembly with ONT reads...\"\nsingularity exec ${FLYE_CONTAINER} flye --nano-raw /data/CAN-290_ONT.fastq.gz \\\n     --out-dir /results/hybrid_assembly/results/flye_initial \\\n     --threads 12 \\\n     --iterations 2\n\n# Step 2: Map short reads to the assembly using BWA\n# Mapping the short reads to the draft assembly is essential to identify mismatched regions for polishing\necho \"Step 2: Mapping short reads to assembly with BWA...\"\nsingularity exec ${PP_CONTAINER} bwa index ${OUTPUT_DIR}/flye_initial/assembly.fasta\n\nsingularity exec ${PP_CONTAINER} bwa mem -t 12 ${OUTPUT_DIR}/flye_initial/assembly.fasta \\\n         ${SHORT_R1} &gt; ${OUTPUT_DIR}/aligned_R1.sam\nsingularity exec ${PP_CONTAINER} bwa mem -t 12 ${OUTPUT_DIR}/flye_initial/assembly.fasta \\\n         ${SHORT_R2} &gt; ${OUTPUT_DIR}/aligned_R2.sam\n\n# Step 3: Filter alignments for Polypolish\n# By filtering the alignments, we ensure that only high-quality mappings are used for polishing\necho \"Step 3: Filtering alignments for Polypolish...\"\nsingularity exec ${PP_CONTAINER} polypolish filter \\\n    --in1 ${OUTPUT_DIR}/aligned_R1.sam --in2 ${OUTPUT_DIR}/aligned_R2.sam \\\n    --out1 ${OUTPUT_DIR}/filtered_R1.sam --out2 ${OUTPUT_DIR}/filtered_R2.sam \n\n# Step 4: Polish the assembly with Polypolish\n# Finally, we use Polypolish to correct errors in the assembly based on the filtered short-read alignments\necho \"Step 4: Polishing assembly with Polypolish...\"\nsingularity exec ${PP_CONTAINER} polypolish polish \\\n    ${OUTPUT_DIR}/flye_initial/assembly.fasta \\\n    ${OUTPUT_DIR}/filtered_R1.sam ${OUTPUT_DIR}/filtered_R2.sam &gt; ${OUTPUT_DIR}/${SAMPLE_NAME}.fasta\n\necho \"Hybrid assembly completed: ${SAMPLE_NAME}.fasta\"\n# Submit the hybrid assembly job\nsbatch hybrid_assembly_job.slurm",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#comparing-three-assembly-approaches",
    "href": "practicals/Practicals - WGS/PRACTICAL_ASSEMBLY_AND_MAPPING.html#comparing-three-assembly-approaches",
    "title": "Practical 2: De novo Assembly and Mapping",
    "section": "7.3 Comparing Three Assembly Approaches",
    "text": "7.3 Comparing Three Assembly Approaches\nAfter the hybrid assembly completes, you can compare all three approaches:\n#!/bin/bash\n#SBATCH -A C3SE408-25-2\n#SBATCH -J quast_all\n#SBATCH -p vera\n#SBATCH -N 1 --cpus-per-task=4\n#SBATCH -t 01:00:00\n#SBATCH --output=/cephyr/users/ktor/Vera/practical_2/logs/quast_all_%j.out\n#SBATCH --error=/cephyr/users/ktor/Vera/practical_2/logs/quast_all_%j.err\n\n# Set paths - ADJUST THESE TO YOUR ACTUAL PATHS\nCONTAINER_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/BIO511/singularity_images/quast.sif\"\nRESULTS_PATH=\"/cephyr/NOBACKUP/groups/n2bin_gu/students/yourname/practical_2\"\nOUTPUT_DIR=\"${RESULTS_PATH}/results/quast_all_results\"\n\n# Set the path to the assemblies\nASSEMBLIES_DIR=\"${RESULTS_PATH}/assembly\"\nASSEMBLIES_DIR_HYBRID=\"${RESULTS_PATH}/hybrid_assembly/results\"\n\n# Bind paths for container\nexport SINGULARITY_BINDPATH=\"${ASSEMBLIES_DIR}:/assemblies,${ASSEMBLIES_DIR_HYBRID}:/assemblies_hybrid,${OUTPUT_DIR}:/output\"\n\nmkdir -p ${OUTPUT_DIR}\n\n# Run QUAST on all three assemblies\nsingularity exec ${CONTAINER_PATH} quast.py \\\n    -o ${OUTPUT_DIR} \\\n    --threads 4 \\\n    --plots-format png \\\n    --labels \"Basic,Polished,Hybrid\" \\\n    /assemblies/flye_basic/assembly.fasta \\\n    /assemblies/flye_polished/assembly.fasta \\\n    /assemblies_hybrid/hybrid_assembly.fasta\n\necho \"Complete QUAST analysis finished\"\n\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nAssembly vs. Mapping: Choose based on your research questions and available references, in many cases both are needed or useful\nQuality Assessment: Multiple metrics are needed to evaluate assembly and mapping quality, however bigger is not always better (such as in the case when nanopore introduces insertions/deletions)\nTool Selection: Multiple tools exist for assembly and mapping, differing in algorithms and performance; choose based on data type and research needs.\nIterative Process: Assembly and mapping often require parameter optimization and multiple iterations of the same workflow, also polishing steps may sometimes need more or less iterations to generate balanced results\nComprehensive Reporting: As assemblies are rich in data, tools like MultiQC help gather results for quick and big picture interpretation",
    "crumbs": [
      "Practical 2: Genome Assembly and Mapping"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html",
    "title": "Practical 3: Python Functions",
    "section": "",
    "text": "Functions are everywhere in python and its time for you to start using them to take you to the next level!\nIf there are any built-in python functions you have not seen or heard about before (like for example isdigit(), max() etc.), search online for the documentation. The official page python.docs.org has information on the built-in functionality that comes with the standard python installation.\n\nRemember that you have to both define a function using the def block and then later call the function itself to have the code it contains acutally run. Calling a function means running the fucntion.\nAlso do not forget that the function has its own namespace, meaning code outside the funtion can not access the variables that only exist inside the function. The function is meant to be sectioned off. The data you want to access outside the function needs to be returned",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#defining-a-function",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#defining-a-function",
    "title": "Practical 3: Python Functions",
    "section": "3.1 Defining a function",
    "text": "3.1 Defining a function\ndef add_two_numbers(num_one, num_two):\n    number_to_return = num_one + num_two\n    return number_to_return",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#calling-the-function",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#calling-the-function",
    "title": "Practical 3: Python Functions",
    "section": "3.2 Calling the function",
    "text": "3.2 Calling the function\nmy_added_numbers = add_two_numbers(1, 1)\n\nprint(my_added_numbers) # will print 2",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section",
    "title": "Practical 3: Python Functions",
    "section": "4.1 ",
    "text": "4.1 \n# Count-above  (uses nums, limit, count)\n# Goal: make a small function called whatever you want and then call(run) it.\n\n# a) Define a function named count_above that takes two arguments: seq and lim.\n\n# b) Inside the function, create a LOCAL variable named count starting at 0.\n\n# c) Loop through seq; for each number strictly greater than lim, increase count by 1.\n\n# d) Return count.\n\n# e) Outside the function:\n#    - Print the GLOBAL count.\n#    - Call count_above(nums, limit) and print the returned number.\n#    - Print the GLOBAL count again (notice the global didn’t change).\n\n# Your code below",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section-1",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section-1",
    "title": "Practical 3: Python Functions",
    "section": "4.2 ",
    "text": "4.2 \n# Text summary  (uses text, summary)\n# Goal: classify characters with an if/elif/else chain and return a clear result.\n\n# a) Define a function named summarize_text that takes one argument: s.\n\n# b) Inside the function, create a LOCAL variable named summary that holds a result dictionary\n#    with exactly these keys: \"digits\", \"letters\", \"other\" — each starting at 0.\n\n# c) Loop through each character in s:\n#       - if the character is a digit, increase \"digits\"\n#       - elif the character is a letter, increase \"letters\"\n#       - else increase \"other\"\n\n# d) Return the summary dictionary.\n\n# e) Outside the function:\n#    - Print the GLOBAL summary.\n#    - Call summarize_text(text) and print the returned dictionary.\n#    - Print the GLOBAL summary again.\n\n# Your code below",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section-2",
    "href": "practicals/Practicals - Python/PRACTICAL_FUNCTIONS.html#section-2",
    "title": "Practical 3: Python Functions",
    "section": "4.3 ",
    "text": "4.3 \n#C) Aggregate with mode  (uses nums, limit, result)\n# Goal: nested decisions based on a mode string. Return one final value.\n\n# a) Define a function named aggregate that takes three arguments: seq, mode, threshold.\n\n# b) Inside the function, create a LOCAL variable named result.\n#    Initialize it based on mode:\n#       - if mode is \"sum\": start at 0\n#       - if mode is \"count\": start at 0\n#       - if mode is \"max\": start at None (meaning “no qualifying value yet”)\n\n# c) Loop through each number n in seq:\n#       - First, ignore n if it is negative (skip it).\n#       - If n is at least threshold, then:\n#           * if mode is \"sum\": add n to result\n#           * elif mode is \"count\": increase result by 1\n#           * else (treat any other mode as \"max\"):\n#                 if result is None or n is greater than result, update result to n\n\n# d) Return the result.\n\n# e) Outside the function:\n#    - Print the GLOBAL result.\n#    - Call and print each of these:\n#         aggregate(nums, \"sum\", limit)\n#         aggregate(nums, \"count\", limit)\n#         aggregate(nums, \"max\", limit)\n#    - Print the GLOBAL result again.\n\n# Your Code below",
    "crumbs": [
      "Practical 3: Python Functions"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_PLOTTING.html",
    "href": "practicals/Practicals - Python/PRACTICAL_PLOTTING.html",
    "title": "Practical 6: Python Plotting",
    "section": "",
    "text": "1 Introduction\nThe instructions to this practical is not completely finsihed yet. In the end, it will contain something like this:\n\n\n2 Exercises\n\n2.0.1 6.1\nTry plotting scatterplots of the seaborn dataset “Penguins”.\na.) use different colors based on species or island b.)create one with matplotlib or seaborn, plotly and plotting\n\n\n2.0.2 6.2\nTry to create a subplot with three plots"
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_LOOPING.html",
    "href": "practicals/Practicals - Python/PRACTICAL_LOOPING.html",
    "title": "Practical 2: Python Loops",
    "section": "",
    "text": "Looping over different types of sequences is key in most programming languages and python is no different. Today the exercises will involve creating some simple loops followed by some more invluved loop constructs where you will combine looping with the if clauses we learned yesterday.",
    "crumbs": [
      "Practical 2: Python Loops"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_LOOPING.html#simple-loops",
    "href": "practicals/Practicals - Python/PRACTICAL_LOOPING.html#simple-loops",
    "title": "Practical 2: Python Loops",
    "section": "3.1 Simple loops",
    "text": "3.1 Simple loops\n\ncreate a list of 7 items. Loop over the list printing each item of the list.\nFor each iteration, print the loop number (index). E.g. on the third loop (iteration) its supposed to print a “3”.\nAdd an If-statement to make the loop stop after printing its 5th item.",
    "crumbs": [
      "Practical 2: Python Loops"
    ]
  },
  {
    "objectID": "practicals/Practicals - Python/PRACTICAL_LOOPING.html#nested-loops",
    "href": "practicals/Practicals - Python/PRACTICAL_LOOPING.html#nested-loops",
    "title": "Practical 2: Python Loops",
    "section": "3.2 Nested loops",
    "text": "3.2 Nested loops\nExample code: Here’s a simple nested loop, looking for codons in sequences\nsequences = ['ATCTGAGTCCACACATG', 'GCGTCGTGCGATGTTCACGTTGAT', 'CAGTAGTACTCAGT', 'GGTATGCTAGACGAGATCTAATA']\ncodons = ['CCA', 'TGT', 'GTA', 'TAG']\nfor sequence in sequences:\n  for codon in codons:\n    if codon in sequence:\n      print(codon + \" is in \" + sequence)\nSee if you can follow every line and work out how this example code works.\nWhen working with loops within loops it can get a bit tricky to keep track of things. A tip is to write the steps down in plain text before writing the actual code.\n\n3.2.1 \nCreate a nested for loop as in the example which looks for start and stop codons in the sequences list from the example above. Which sequences have both a start and a stop codon?\n\n\n3.2.2 \nTry to find a way to make sure that the start codon is before the stop codon in a sequence. Which sequence(s) have a start codon before a stop codon? We can loop through a dictionary by defining two variables: both the key and value.\nExample: Build a simple dict with the keys pat_001, pat_002 and pat_003 that has a value each. Here, the values are list:\ndata = dict({'pat_001': ['bacZZt98', 'bac889Ytd'], 'pat_002': ['bac0GFrr'], 'pat_003': ['bac889Ytd', 'bacFq55Hj', 'bacZZt98']})\n\n# Loop through the dict printing each key and each value as a list.\nfor key_patient, value_bact_list in data.items():  \n  print(key_patient)\n  print(value_bact_list)\n\n# add a line to see if the value (list) has the bacterial strain 'bac889Ytd'. \n# If it does it should return 'True'. If not, it should say 'False'.\n\nfor key_patient, value_bact_list in data.items():  \n  print(key_patient)\n  print(value_bact_list)\n  'bac889Ytd' in value_bact_list\n\n\n3.2.3 \nMake a loop going through the dictionary saved as data, given in the example. Use the loop to create a list of all the unique bacteria. Start by creating an empty list. Then create a for-loop where, for each iteration, you add the current bacteria to the list. Next, introduce an if-statement. Before adding each bacteria, use the if-statement to check whether the bacteria is or is not already in the list. If it’s not in the list, add it (Hint: use the append() command). Otherwise, continue to the next bacteria. See if you can use a search engine (like google) to figure out how to append items to a list.\n\n\n3.2.4 \nLets do the same thing but instead add each unique bacteria as a key in a dictionary. The values can be empty lists. Start by making an empty dictionary. Hint: Search online for how to list the keys in a dict, in order to compare each bacteria to the dict you’re building.\n\n\n3.2.5 \nNow let’s append the patients to the lists of each key. The final dict should look like this: ``python {'bacZZt98': ['pat_001', 'pat_003'], 'bac889Ytd': ['pat_001', 'pat_003'], 'bac0GFrr': ['pat_002'], 'bacFq55Hj': ['pat_003']}",
    "crumbs": [
      "Practical 2: Python Loops"
    ]
  }
]